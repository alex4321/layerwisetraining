{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written using Llama self attention module as example.\n",
    "\n",
    "### Original self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote\n",
    "\n",
    "- $B$ is batch size\n",
    "- $S$ is a sequence length\n",
    "- $D$ is a hidden size\n",
    "\n",
    "- and we have $X \\in \\mathbb{R}^{B, S, D}$ input.\n",
    "\n",
    "Let's denote parameters:\n",
    "- $W_Q \\in \\mathbb{R}^{D x (H d_h)}$ - query projection\n",
    "- $W_K \\in \\mathbb{R}^{D x (H d_h)}$ - key projection\n",
    "- $W_V \\in \\mathbb{R}^{D x (H d_h)}$ - value projection\n",
    "- $W_O \\in \\mathbb{R}^{D x (H d_h)}$ - output projection\n",
    "- where $H$ is head count and $d_h$ is individual head dimension\n",
    "- also, $H_k$ is a number of key-value heads\n",
    "\n",
    "Now let's write down query / key / value projections:\n",
    "\n",
    "- $Q = X W_Q, Q \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "- $K = X W_K, K \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "- $V = X W_V, V \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "\n",
    "Now let's transpose them for multihead attention:\n",
    "- $Q_{transposed} = Q.view(B, S, H, d_h).transpose(1, 2), Q_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "- $K_{transposed} = K.view(B, S, H, d_h).transpose(1, 2), K_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "- $V_{transposed} = V.view(B, S, H, d_h).transpose(1, 2), V_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "\n",
    "Now let's write down RoPE (rotary position embeddings)\n",
    "\n",
    "- $rotateHalf(X) = (-X_{:, :, :, d_h/2:}, X_{:, :, :, :d_h/2})$\n",
    "- $Q_{rotated} = Q_{transposed} cos(\\theta) + rotateHalf(Q_{transposed}) sin(\\theta)$\n",
    "- $K_{rotated} = K_{transposed} cos(\\theta) + rotateHalf(K_{transposed}) sin(\\theta)$\n",
    "\n",
    "```python\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "```\n",
    "\n",
    "\n",
    "Assuming $H=H_k$ no repeat occurs, so attention directly use $Q_{rotated}$ and $K_{rotated}$\n",
    "\n",
    "Now to attention scores:\n",
    "\n",
    "- $A = { {Q_{rotated} K_{rotated}^T} \\over {\\sqrt{d_h}} } + M$, where $M$ is a causal mask with large negative values for masked positions\n",
    "- Now applying softmax and dropout:\n",
    "\n",
    "  $ A_{postprocessed} = dropout(softmax(A, dim=-1), p)$ where we can do dropout through elemenwise multiplication to random matrix.\n",
    "\n",
    "  So $ A_{postprocessed} = softmax(A, dim=-1) * (random(S, S) < p)$\n",
    "\n",
    "Weighted sum:\n",
    "\n",
    "- $O = A_{postprocessed} V = A_{postprocessed} (X W_V)$\n",
    "- $O_{reshaped} = O.view((B, S, D))$\n",
    "\n",
    "Than final output is \n",
    "\n",
    "$Y = O_{reshaped} W_O$\n",
    "\n",
    "### Attention simpliciation idea\n",
    "\n",
    "But we can (formally incorrectly, I am just checking if the idea will be a good approximation) to think about $A$ as a linear attention, this way replacing:\n",
    "\n",
    "- $A = { {Q_{rotated} K_{rotated}^T} \\over {\\sqrt{d_h}}}$\n",
    "- $A_{postprocessed} = A * (random(S, S) < p)$\n",
    "\n",
    "Now, assuming that all we know after attention computation is two matrices (but we know query / key / value projections other internal stuff, it is just attention mechanism what we don't want to recompute):\n",
    "- $A_{postprocessed}$ \n",
    "- $O$\n",
    "\n",
    "Now, assuming we used these replaced attention mechanism during forward pass.\n",
    "\n",
    "Assuming we have this loss function: $Loss(Y, X) = |Y_{:, :-1, :} - X_{:, 1:, :}|$ (expecting $Y$ and $X$ to be $(B, S, D)$ shape matrixes).\n",
    "\n",
    "We need weight ($W_K$ , $W_Q$ , $W_V$ , $W_O$) gradients to be computed reusing inputs / intermediates / $A_{postprocessed} / $O$ / outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    d = x.shape[-1]\n",
    "    return torch.cat((-x[..., d//2:], x[..., :d//2]), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    q_rotated = q * cos + rotate_half(q) * sin\n",
    "    k_rotated = k * cos + rotate_half(k) * sin\n",
    "    return q_rotated, k_rotated\n",
    "\n",
    "def get_rotary_embeddings(seq_len: int, dim: int, theta: float = 10000.0):\n",
    "    position = torch.arange(seq_len, dtype=torch.float32)\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim, 2).float() / dim)\n",
    "    angles = position.unsqueeze(1) * freqs.unsqueeze(0)  # (S, dim/2)\n",
    "    \n",
    "    cos = torch.cos(angles)  # (S, dim/2)\n",
    "    sin = torch.sin(angles)  # (S, dim/2)\n",
    "    \n",
    "    # Expand to match the shape of Q/K (B, H, S, d_h)\n",
    "    cos = cos.view(1, 1, seq_len, -1)  # (1, 1, S, d_h/2)\n",
    "    sin = sin.view(1, 1, seq_len, -1)\n",
    "    \n",
    "    # Concatenate to handle even/odd dimensions properly\n",
    "    cos = torch.cat([cos, cos], dim=-1)\n",
    "    sin = torch.cat([sin, sin], dim=-1)\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb_backward(grad_rotated, cos, sin):\n",
    "    # Gradient through RoPE for K\n",
    "    grad_transposed = grad_rotated * cos + rotate_half(grad_rotated) * sin\n",
    "    return grad_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LlamaLikeLinearAttentionState:\n",
    "    Y: torch.Tensor\n",
    "    O_reshaped: torch.Tensor\n",
    "    A_postprocessed: torch.Tensor\n",
    "    V_transposed: torch.Tensor\n",
    "    K_rotated: torch.Tensor\n",
    "    Q_rotated: torch.Tensor\n",
    "    cos: torch.Tensor\n",
    "    sin: torch.Tensor\n",
    "\n",
    "\n",
    "def llamalike_linear_attention_forward(X, W_Q, W_K, W_V, W_O, H, cos, sin, dropout_mask):\n",
    "    B, S, D = X.shape\n",
    "    d_h = D // H\n",
    "\n",
    "    # Forward pass\n",
    "    Q = torch.matmul(X, W_Q)  # (B, S, H*d_h)\n",
    "    K = torch.matmul(X, W_K)\n",
    "    V = torch.matmul(X, W_V)\n",
    "\n",
    "    Q_transposed = Q.view(B, S, H, d_h).transpose(1, 2)  # (B, H, S, d_h)\n",
    "    K_transposed = K.view(B, S, H, d_h).transpose(1, 2)\n",
    "    V_transposed = V.view(B, S, H, d_h).transpose(1, 2)\n",
    "\n",
    "    # Apply RoPE with precomputed cos/sin\n",
    "    Q_rotated, K_rotated = apply_rotary_pos_emb(Q_transposed, K_transposed, cos, sin)\n",
    "\n",
    "    # Rest of forward pass remains the same...\n",
    "    A = (torch.matmul(Q_rotated, K_rotated.transpose(-1, -2)) / (d_h**0.5))  # Scaled by sqrt(d_h)\n",
    "                                                                             # (B, H, S, d_h) * (B, H, d_h, S) -> (B, H, S, S)\n",
    "    A_postprocessed = A * dropout_mask  # Apply dropout mask\n",
    "\n",
    "    O = torch.matmul(A_postprocessed, V_transposed) # (B, H, S, S) * (B, H, S, d_h) -> (B, H, S, d_h)\n",
    "    O_reshaped = O.transpose(1, 2).reshape(1, 2, 2) # (B, H, S, d_h) -> (B, S, H, d_h) -> (B, S, D)\n",
    "    Y = torch.matmul(O_reshaped, W_O) # (B, S, D) * (D, D) -> (B, S, D)\n",
    "\n",
    "    return LlamaLikeLinearAttentionState(\n",
    "        Y=Y,\n",
    "        O_reshaped=O_reshaped,\n",
    "        A_postprocessed=A_postprocessed,\n",
    "        V_transposed=V_transposed,\n",
    "        K_rotated=K_rotated,\n",
    "        Q_rotated=Q_rotated,\n",
    "        cos=cos,\n",
    "        sin=sin\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamalike_linear_attention_gradients(state, X, d_h):\n",
    "    B, S, D = X.shape\n",
    "    H = D // d_h\n",
    "\n",
    "    dLoss_dY = torch.sign(state.Y[:, :-1, :] - X[:, 1:, :])  # Gradient of L1 loss\n",
    "    manual_grad_W_O = torch.einsum('bsd,bse->de', state.O_reshaped[:, :-1, :], dLoss_dY)\n",
    "\n",
    "    # Expand to full sequence length with zeros\n",
    "    dLoss_dY_full = torch.zeros_like(state.Y)          # Shape: (B, S, D)\n",
    "    dLoss_dY_full[:, :-1, :] = dLoss_dY                # Fill first S-1 positions\n",
    "    \n",
    "    # Gradient for O_reshaped: dLoss_dY_full @ W_O^T\n",
    "    dLoss_dO_reshaped = torch.matmul(dLoss_dY_full, W_O.T)  # (B, S, D) * (D, D) -> (B, S, D)\n",
    "    # Reshape and transpose to match O’s original shape (B, H, S, d_h)\n",
    "    dLoss_dO = dLoss_dO_reshaped.view(B, S, H, d_h).transpose(1, 2)  # (B, H, S, d_h)\n",
    "    \n",
    "    # Now compute dLoss/dV_transposed\n",
    "    dLoss_dV_transposed = torch.matmul(state.A_postprocessed.transpose(-1, -2), dLoss_dO)  # (B, H, S, S) * (B, H, S, d_h) -> (B, H, S, d_h)\n",
    "    dLoss_dV = dLoss_dV_transposed.transpose(1, 2).reshape(B, S, D)  # (B, H, S, d_h) -> (B, S, D)\n",
    "\n",
    "    # Manual gradient for W_V\n",
    "    manual_grad_W_V = torch.einsum('bsd,bse->de', X, dLoss_dV)\n",
    "\n",
    "    dLoss_dA = torch.matmul(dLoss_dO, state.V_transposed.transpose(-1, -2)) * dropout_mask  # (B, H, S, d_h) * (B, H, d_h, S) -> (B, H, S, S)\n",
    "\n",
    "    # Backprop through A = Q_rotated @ K_rotated^T / sqrt(d_h)\n",
    "    dLoss_dQ_rotated = torch.matmul(dLoss_dA, state.K_rotated) / (d_h**0.5)  # Apply scaling here\n",
    "    # Backprop through RoPE for Q (correct sign)\n",
    "    dLoss_dQ_transposed = dLoss_dQ_rotated * state.cos - rotate_half(dLoss_dQ_rotated) * state.sin\n",
    "\n",
    "    # Reshape and compute dLoss/dW_Q\n",
    "    dLoss_dQ = dLoss_dQ_transposed.transpose(1, 2).reshape(1, 2, 2)\n",
    "    manual_grad_W_Q = torch.einsum('bsd,bse->de', X, dLoss_dQ)\n",
    "\n",
    "    # Backprop through A = Q_rotated @ K_rotated^T / sqrt(d_h)\n",
    "    # Corrected transpose after matmul and sign in RoPE backward\n",
    "    dLoss_dK_rotated = torch.matmul(state.Q_rotated.transpose(-1, -2), dLoss_dA).transpose(-1, -2) / (d_h**0.5)\n",
    "    # Backprop through RoPE for K (FIXED SIGN HERE)\n",
    "    dLoss_dK_transposed = dLoss_dK_rotated * state.cos + rotate_half(dLoss_dK_rotated) * state.sin  # Corrected to \"+\"\n",
    "    \n",
    "    # Reshape and compute dLoss/dW_K\n",
    "    dLoss_dK = dLoss_dK_transposed.transpose(1, 2).reshape(B, S, D)\n",
    "    manual_grad_W_K = torch.einsum('bsd,bse->de', X, dLoss_dK)\n",
    "    \n",
    "    return manual_grad_W_Q, manual_grad_W_K, manual_grad_W_V, manual_grad_W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Inputs\n",
    "B = 1\n",
    "S = 2\n",
    "D = 2\n",
    "H = 1\n",
    "d_h = 2\n",
    "dropout_p = 0.5\n",
    "\n",
    "X = torch.randn(B, S, D)  # (B, S, D)\n",
    "Y_target = X[:, 1:, :]    # Loss compares Y[:, :-1] to X[:, 1:]\n",
    "\n",
    "# Parameters\n",
    "W_Q = torch.randn(D, D, requires_grad=True)\n",
    "W_K = torch.randn(D, D, requires_grad=True)\n",
    "W_V = torch.randn(D, D, requires_grad=True)\n",
    "W_O = torch.randn(D, D, requires_grad=True)\n",
    "\n",
    "# Fixed dropout mask (for reproducibility)\n",
    "dropout_mask = (torch.rand(S, D) > dropout_p).float()  # Binary mask\n",
    "\n",
    "# Example for S=2, d_h=2\n",
    "cos, sin = get_rotary_embeddings(seq_len=S, dim=D, theta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = llamalike_linear_attention_forward(X, W_Q, W_K, W_V, W_O, H, cos, sin, dropout_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = llamalike_linear_attention_gradients(state, X, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grad_W_Q, manual_grad_W_K, manual_grad_W_V, manual_grad_W_O = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.abs(state.Y[:, :-1, :] - X[:, 1:, :]).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_O, W_O.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_Q, W_Q.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_V, W_V.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_K, W_K.grad, atol=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
