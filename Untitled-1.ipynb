{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is written using Llama self attention module as example.\n",
    "\n",
    "### Original self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's denote\n",
    "\n",
    "- $B$ is batch size\n",
    "- $S$ is a sequence length\n",
    "- $D$ is a hidden size\n",
    "\n",
    "- and we have $X \\in \\mathbb{R}^{B, S, D}$ input.\n",
    "\n",
    "Let's denote parameters:\n",
    "- $W_Q \\in \\mathbb{R}^{D x (H d_h)}$ - query projection\n",
    "- $W_K \\in \\mathbb{R}^{D x (H d_h)}$ - key projection\n",
    "- $W_V \\in \\mathbb{R}^{D x (H d_h)}$ - value projection\n",
    "- $W_O \\in \\mathbb{R}^{D x (H d_h)}$ - output projection\n",
    "- where $H$ is head count and $d_h$ is individual head dimension\n",
    "- also, $H_k$ is a number of key-value heads\n",
    "\n",
    "Now let's write down query / key / value projections:\n",
    "\n",
    "- $Q = X W_Q, Q \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "- $K = X W_K, K \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "- $V = X W_V, V \\in \\mathbb{R}^{B x S x (H d_h)}$\n",
    "\n",
    "Now let's transpose them for multihead attention:\n",
    "- $Q_{transposed} = Q.view(B, S, H, d_h).transpose(1, 2), Q_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "- $K_{transposed} = K.view(B, S, H, d_h).transpose(1, 2), K_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "- $V_{transposed} = V.view(B, S, H, d_h).transpose(1, 2), V_{transposed} \\in \\mathbb{R}^{B x H x S x d_h}$\n",
    "\n",
    "Now let's write down RoPE (rotary position embeddings)\n",
    "\n",
    "- $rotateHalf(X) = (-X_{:, :, :, d_h/2:}, X_{:, :, :, :d_h/2})$\n",
    "- $Q_{rotated} = Q_{transposed} cos(\\theta) + rotateHalf(Q_{transposed}) sin(\\theta)$\n",
    "- $K_{rotated} = K_{transposed} cos(\\theta) + rotateHalf(K_{transposed}) sin(\\theta)$\n",
    "\n",
    "```python\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "```\n",
    "\n",
    "\n",
    "Assuming $H=H_k$ no repeat occurs, so attention directly use $Q_{rotated}$ and $K_{rotated}$\n",
    "\n",
    "Now to attention scores:\n",
    "\n",
    "- $A = { {Q_{rotated} K_{rotated}^T} \\over {\\sqrt{d_h}} } + M$, where $M$ is a causal mask with large negative values for masked positions\n",
    "- Now applying softmax and dropout:\n",
    "\n",
    "  $ A_{postprocessed} = dropout(softmax(A, dim=-1), p)$ where we can do dropout through elemenwise multiplication to random matrix.\n",
    "\n",
    "  So $ A_{postprocessed} = softmax(A, dim=-1) * (random(S, S) < p)$\n",
    "\n",
    "Weighted sum:\n",
    "\n",
    "- $O = A_{postprocessed} V = A_{postprocessed} (X W_V)$\n",
    "- $O_{reshaped} = O.view((B, S, D))$\n",
    "\n",
    "Than final output is \n",
    "\n",
    "$Y = O_{reshaped} W_O$\n",
    "\n",
    "### Attention simpliciation idea\n",
    "\n",
    "But we can (formally incorrectly, I am just checking if the idea will be a good approximation) to think about $A$ as a linear attention, this way replacing:\n",
    "\n",
    "- $A = { {Q_{rotated} K_{rotated}^T} \\over {\\sqrt{d_h}}}$\n",
    "- $A_{postprocessed} = A * (random(S, S) < p)$\n",
    "\n",
    "Now, assuming that all we know after attention computation is two matrices (but we know query / key / value projections other internal stuff, it is just attention mechanism what we don't want to recompute):\n",
    "- $A_{postprocessed}$ \n",
    "- $O$\n",
    "\n",
    "Now, assuming we used these replaced attention mechanism during forward pass.\n",
    "\n",
    "Assuming we have this loss function: $Loss(Y, X) = |Y_{:, :-1, :} - X_{:, 1:, :}|$ (expecting $Y$ and $X$ to be $(B, S, D)$ shape matrixes).\n",
    "\n",
    "We need weight ($W_K$ , $W_Q$ , $W_V$ , $W_O$) gradients to be computed reusing inputs / intermediates / $A_{postprocessed} / $O$ / outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x):\n",
    "    d = x.shape[-1]\n",
    "    return torch.cat((-x[..., d//2:], x[..., :d//2]), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    q_rotated = q * cos + rotate_half(q) * sin\n",
    "    k_rotated = k * cos + rotate_half(k) * sin\n",
    "    return q_rotated, k_rotated\n",
    "\n",
    "def get_rotary_embeddings(seq_len: int, dim: int, theta: float = 10000.0):\n",
    "    position = torch.arange(seq_len, dtype=torch.float32)\n",
    "    freqs = theta ** (-2 * torch.arange(0, dim, 2).float() / dim)\n",
    "    angles = position.unsqueeze(1) * freqs.unsqueeze(0)  # (S, dim/2)\n",
    "    \n",
    "    cos = torch.cos(angles)  # (S, dim/2)\n",
    "    sin = torch.sin(angles)  # (S, dim/2)\n",
    "    \n",
    "    # Expand to match the shape of Q/K (B, H, S, d_h)\n",
    "    cos = cos.view(1, 1, seq_len, -1)  # (1, 1, S, d_h/2)\n",
    "    sin = sin.view(1, 1, seq_len, -1)\n",
    "    \n",
    "    # Concatenate to handle even/odd dimensions properly\n",
    "    cos = torch.cat([cos, cos], dim=-1)\n",
    "    sin = torch.cat([sin, sin], dim=-1)\n",
    "    return cos, sin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb_backward(grad_rotated, cos, sin):\n",
    "    # Gradient through RoPE for K\n",
    "    grad_transposed = grad_rotated * cos + rotate_half(grad_rotated) * sin\n",
    "    return grad_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class LlamaLikeLinearAttentionState:\n",
    "    Y: torch.Tensor\n",
    "    O_reshaped: torch.Tensor\n",
    "    A_postprocessed: torch.Tensor\n",
    "    V_transposed: torch.Tensor\n",
    "    K_rotated: torch.Tensor\n",
    "    Q_rotated: torch.Tensor\n",
    "    cos: torch.Tensor\n",
    "    sin: torch.Tensor\n",
    "    W_O: torch.Tensor\n",
    "    W_Q: torch.Tensor\n",
    "    W_K: torch.Tensor\n",
    "    W_V: torch.Tensor\n",
    "    dropout_mask: torch.Tensor\n",
    "\n",
    "\n",
    "def llamalike_linear_attention_forward(X, W_Q, W_K, W_V, W_O, H, cos, sin, dropout_mask):\n",
    "    B, S, D = X.shape\n",
    "    d_h = D // H\n",
    "    assert D == H * d_h, f\"Hidden size {D} must be divisible by {H} heads\"\n",
    "\n",
    "    # Projections\n",
    "    Q = X @ W_Q  # (B, S, D)\n",
    "    K = X @ W_K\n",
    "    V = X @ W_V\n",
    "\n",
    "    # Reshape for multi-head attention\n",
    "    Q_transposed = Q.view(B, S, H, d_h).transpose(1, 2)  # (B, H, S, d_h)\n",
    "    K_transposed = K.view(B, S, H, d_h).transpose(1, 2)\n",
    "    V_transposed = V.view(B, S, H, d_h).transpose(1, 2)\n",
    "\n",
    "    # Apply RoPE\n",
    "    Q_rotated, K_rotated = apply_rotary_pos_emb(Q_transposed, K_transposed, cos, sin)\n",
    "\n",
    "    # Attention scores\n",
    "    A = (Q_rotated @ K_rotated.transpose(-1, -2)) / (d_h**0.5)\n",
    "    A_postprocessed = A * dropout_mask  # Apply dropout\n",
    "\n",
    "    # Output computation\n",
    "    O = A_postprocessed @ V_transposed  # (B, H, S, d_h)\n",
    "    O_reshaped = O.transpose(1, 2).reshape(B, S, D)  # Dynamic reshape\n",
    "    Y = O_reshaped @ W_O\n",
    "\n",
    "    return LlamaLikeLinearAttentionState(\n",
    "        Y=Y,\n",
    "        O_reshaped=O_reshaped,\n",
    "        A_postprocessed=A_postprocessed,\n",
    "        V_transposed=V_transposed,\n",
    "        K_rotated=K_rotated,\n",
    "        Q_rotated=Q_rotated,\n",
    "        cos=cos,\n",
    "        sin=sin,\n",
    "        W_O=W_O,\n",
    "        W_Q=W_Q,\n",
    "        W_K=W_K,\n",
    "        W_V=W_V,\n",
    "        dropout_mask=dropout_mask\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llamalike_linear_attention_gradients(state, X, d_h):\n",
    "    B, S, D = X.shape\n",
    "    H = D // d_h\n",
    "    \n",
    "    # Gradient through output projection\n",
    "    dLoss_dY = torch.sign(state.Y[:, :-1, :] - X[:, 1:, :])\n",
    "    dLoss_dO_reshaped = torch.zeros_like(state.Y)\n",
    "    dLoss_dO_reshaped[:, :-1, :] = dLoss_dY @ state.W_O.T\n",
    "    \n",
    "    # Gradient through attention output\n",
    "    dLoss_dO = dLoss_dO_reshaped.view(B, S, H, d_h).transpose(1, 2)\n",
    "    dLoss_dV_transposed = state.A_postprocessed.transpose(-1, -2) @ dLoss_dO\n",
    "    dLoss_dV = dLoss_dV_transposed.transpose(1, 2).reshape(B, S, D)\n",
    "    \n",
    "    # Gradients for value projection (sum over batch)\n",
    "    manual_grad_W_V = (X.transpose(1, 2) @ dLoss_dV).sum(dim=0)  # Sum batch\n",
    "\n",
    "    # Backprop through attention scores\n",
    "    dLoss_dA = (dLoss_dO @ state.V_transposed.transpose(-1, -2)) * state.dropout_mask\n",
    "    \n",
    "    # Gradients for query and key projections\n",
    "    dLoss_dQ_rotated = (dLoss_dA @ state.K_rotated) / (d_h**0.5)\n",
    "    dLoss_dK_rotated = (dLoss_dA.transpose(-1, -2) @ state.Q_rotated) / (d_h**0.5)\n",
    "    \n",
    "    # Backprop through RoPE\n",
    "    dLoss_dQ_transposed = dLoss_dQ_rotated * state.cos - rotate_half(dLoss_dQ_rotated) * state.sin\n",
    "    dLoss_dK_transposed = dLoss_dK_rotated * state.cos + rotate_half(dLoss_dK_rotated) * state.sin\n",
    "    \n",
    "    # Reshape gradients\n",
    "    dLoss_dQ = dLoss_dQ_transposed.transpose(1, 2).reshape(B, S, D)\n",
    "    dLoss_dK = dLoss_dK_transposed.transpose(1, 2).reshape(B, S, D)\n",
    "    \n",
    "    # Sum gradients over batch\n",
    "    manual_grad_W_Q = (X.transpose(1, 2) @ dLoss_dQ).sum(dim=0)  # Sum batch\n",
    "    manual_grad_W_K = (X.transpose(1, 2) @ dLoss_dK).sum(dim=0)  # Sum batch\n",
    "    manual_grad_W_O = (state.O_reshaped[:, :-1].transpose(1, 2) @ dLoss_dY).sum(dim=0)  # Sum batch\n",
    "\n",
    "    return manual_grad_W_Q, manual_grad_W_K, manual_grad_W_V, manual_grad_W_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Inputs\n",
    "B = 1\n",
    "S = 2\n",
    "D = 8  # Corrected to D=8\n",
    "H = 2\n",
    "d_h = 4  # Now valid: D = H * d_h â†’ 8 = 2*4\n",
    "dropout_p = 0.5\n",
    "\n",
    "X = torch.randn(B, S, D)\n",
    "Y_target = X[:, 1:, :]\n",
    "\n",
    "# Parameters (D=8)\n",
    "W_Q = torch.randn(D, D, requires_grad=True)\n",
    "W_K = torch.randn(D, D, requires_grad=True)\n",
    "W_V = torch.randn(D, D, requires_grad=True)\n",
    "W_O = torch.randn(D, D, requires_grad=True)\n",
    "\n",
    "# Rotary embeddings for per-head dimension\n",
    "cos, sin = get_rotary_embeddings(seq_len=S, dim=d_h, theta=0.1)\n",
    "\n",
    "# Correct dropout mask shape: (B, H, S, S)\n",
    "dropout_mask = (torch.rand(B, H, S, S) > dropout_p).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = llamalike_linear_attention_forward(X, W_Q, W_K, W_V, W_O, H, cos, sin, dropout_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = llamalike_linear_attention_gradients(state, X, d_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_grad_W_Q, manual_grad_W_K, manual_grad_W_V, manual_grad_W_O = grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.abs(state.Y[:, :-1, :] - X[:, 1:, :]).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_O, W_O.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_Q, W_Q.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_V, W_V.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(manual_grad_W_K, W_K.grad, atol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-270.9939,   82.4250,  -40.0187, -132.5305,    8.4649,    0.6771,\n",
       "            5.4119,   -5.7520],\n",
       "        [-186.2232,   62.7531,  -55.4063, -122.5704,  -18.5556,   -1.4843,\n",
       "          -11.8633,   12.6088],\n",
       "        [-127.0943,   38.5447,  -18.2565,  -61.5780,    4.4172,    0.3533,\n",
       "            2.8240,   -3.0015],\n",
       "        [ 273.2087,  -89.2001,   68.2043,  165.0573,   15.7969,    1.2636,\n",
       "           10.0995,  -10.7342],\n",
       "        [-100.1759,   29.1998,   -8.9966,  -42.4486,    8.1920,    0.6553,\n",
       "            5.2374,   -5.5665],\n",
       "        [ 162.9048,  -52.4037,   37.0920,   94.3819,    6.2961,    0.5036,\n",
       "            4.0253,   -4.2783],\n",
       "        [  -2.0288,   -1.5369,    9.5352,   10.1083,    8.6529,    0.6921,\n",
       "            5.5321,   -5.8798],\n",
       "        [ 227.0748,  -68.6936,   31.8294,  109.1288,   -8.5810,   -0.6864,\n",
       "           -5.4861,    5.8309]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_K.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-267.5538,   76.0062,  -58.7720, -136.3131,   -8.4437,    5.5276,\n",
       "            5.4450,   -1.7291],\n",
       "        [-193.7640,   76.8235,  -14.2982, -114.2786,   18.5091,  -12.1168,\n",
       "          -11.9357,    3.7904],\n",
       "        [-125.2992,   35.1952,  -28.0423,  -63.5519,   -4.4061,    2.8844,\n",
       "            2.8413,   -0.9023],\n",
       "        [ 279.6284, -101.1786,   33.2077,  157.9982,  -15.7573,   10.3154,\n",
       "           10.1612,   -3.2269],\n",
       "        [ -96.8468,   22.9879,  -27.1451,  -46.1092,   -8.1714,    5.3494,\n",
       "            5.2694,   -1.6734],\n",
       "        [ 165.4635,  -57.1780,   23.1436,   91.5684,   -6.2803,    4.1114,\n",
       "            4.0499,   -1.2861],\n",
       "        [   1.4877,   -8.0983,   -9.6345,    6.2417,   -8.6312,    5.6504,\n",
       "            5.5659,   -1.7675],\n",
       "        [ 223.5875,  -62.1868,   50.8397,  112.9633,    8.5594,   -5.6034,\n",
       "           -5.5196,    1.7528]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_grad_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-270.9939,   82.4250,  -40.0187, -132.5305,    8.4649,    0.6771,\n",
       "            5.4119,   -5.7520],\n",
       "        [-186.2232,   62.7531,  -55.4063, -122.5704,  -18.5556,   -1.4843,\n",
       "          -11.8633,   12.6088],\n",
       "        [-127.0943,   38.5447,  -18.2565,  -61.5780,    4.4172,    0.3533,\n",
       "            2.8240,   -3.0015],\n",
       "        [ 273.2087,  -89.2001,   68.2043,  165.0573,   15.7969,    1.2636,\n",
       "           10.0995,  -10.7342],\n",
       "        [-100.1759,   29.1998,   -8.9966,  -42.4486,    8.1920,    0.6553,\n",
       "            5.2374,   -5.5665],\n",
       "        [ 162.9048,  -52.4037,   37.0920,   94.3819,    6.2961,    0.5036,\n",
       "            4.0253,   -4.2783],\n",
       "        [  -2.0288,   -1.5369,    9.5352,   10.1083,    8.6529,    0.6921,\n",
       "            5.5321,   -5.8798],\n",
       "        [ 227.0748,  -68.6936,   31.8294,  109.1288,   -8.5810,   -0.6864,\n",
       "           -5.4861,    5.8309]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_K.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.4401e+00, 6.4188e+00, 1.8753e+01, 3.7827e+00, 1.6909e+01, 4.8505e+00,\n",
       "         3.3049e-02, 4.0229e+00],\n",
       "        [7.5408e+00, 1.4070e+01, 4.1108e+01, 8.2918e+00, 3.7065e+01, 1.0633e+01,\n",
       "         7.2445e-02, 8.8184e+00],\n",
       "        [1.7951e+00, 3.3495e+00, 9.7858e+00, 1.9739e+00, 8.8232e+00, 2.5311e+00,\n",
       "         1.7246e-02, 2.0992e+00],\n",
       "        [6.4197e+00, 1.1979e+01, 3.4997e+01, 7.0590e+00, 3.1554e+01, 9.0518e+00,\n",
       "         6.1675e-02, 7.5074e+00],\n",
       "        [3.3291e+00, 6.2118e+00, 1.8149e+01, 3.6607e+00, 1.6363e+01, 4.6941e+00,\n",
       "         3.1983e-02, 3.8932e+00],\n",
       "        [2.5587e+00, 4.7742e+00, 1.3948e+01, 2.8135e+00, 1.2576e+01, 3.6077e+00,\n",
       "         2.4581e-02, 2.9922e+00],\n",
       "        [3.5165e+00, 6.5614e+00, 1.9170e+01, 3.8667e+00, 1.7284e+01, 4.9582e+00,\n",
       "         3.3782e-02, 4.1122e+00],\n",
       "        [3.4872e+00, 6.5068e+00, 1.9010e+01, 3.8345e+00, 1.7140e+01, 4.9170e+00,\n",
       "         3.3502e-02, 4.0780e+00]], grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(W_K.grad - manual_grad_W_K).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1.],\n",
       "          [1., 0.]],\n",
       "\n",
       "         [[0., 1.],\n",
       "          [1., 0.]]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
