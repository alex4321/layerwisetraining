{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "BASE_MODEL = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "DATASET_PATH = \"G:/data/layerwisetraining\"\n",
    "BF16 = torch.cuda.is_bf16_supported()\n",
    "DTYPE = torch.bfloat16 if BF16 else torch.float16\n",
    "DEVICE = \"cuda:0\"\n",
    "DEVICE_ORIGINAL_MODEL = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"Salesforce/wikitext\", \"wikitext-103-raw-v1\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=DTYPE,\n",
    "    device_map=DEVICE_ORIGINAL_MODEL,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    token=HF_TOKEN,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_layers = [\n",
    "    layer\n",
    "    for layer in base_model.model.layers\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings_dataset(ds, tokenizer, directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    texts = ds['text']\n",
    "    for i, text in enumerate(tqdm(texts)):\n",
    "        fname = os.path.join(directory, f\"record-{i}.pkl\")\n",
    "        if os.path.exists(fname):\n",
    "            continue\n",
    "        batch = tokenizer(text, return_tensors=\"pt\")\n",
    "        batch[\"labels\"] = batch[\"input_ids\"]\n",
    "        batch[\"inputs_embeds\"] = base_model.model.embed_tokens(batch[\"input_ids\"])\n",
    "        batch_flatten = {\n",
    "            key: value[0]\n",
    "            for key, value in batch.items()\n",
    "            if key not in [\"input_ids\"]\n",
    "        }\n",
    "        with open(fname, \"wb\") as f:\n",
    "            pickle.dump(batch_flatten, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1835/1835 [00:00<00:00, 57342.35it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_embeddings_dataset(\n",
    "    ds['test'].filter(lambda x: len(x['text']) > 100),\n",
    "    tokenizer,\n",
    "    os.path.join(DATASET_PATH, \"test-layer-0-inputs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 749740/749740 [00:24<00:00, 30213.24it/s]\n"
     ]
    }
   ],
   "source": [
    "convert_to_embeddings_dataset(\n",
    "    ds['train'].filter(lambda x: len(x['text']) > 100),\n",
    "    tokenizer,\n",
    "    os.path.join(DATASET_PATH, \"train-layer-0-inputs\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_to_embeddings_dataset(\n",
    "    ds['train'].filter(lambda x: len(x['text']) > 100).map(lambda x: {'text': x['text'][:128]}),\n",
    "    tokenizer,\n",
    "    os.path.join(DATASET_PATH, \"train-layer-0-inputs-pretrain\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.files = [\n",
    "            os.path.join(directory, fname)\n",
    "            for fname in os.listdir(directory)\n",
    "            if fname.endswith(\".pkl\")\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with open(self.files[idx], \"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_embeddings_train = EmbeddingsDataset(\n",
    "    os.path.join(DATASET_PATH, \"train-layer-0-inputs\")\n",
    ")\n",
    "ds_embeddings_test = EmbeddingsDataset(\n",
    "    os.path.join(DATASET_PATH, \"test-layer-0-inputs\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['attention_mask', 'labels', 'inputs_embeds'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_embeddings_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Get shapes\n",
    "    max_len = max(item['inputs_embeds'].shape[0] for item in batch)\n",
    "    batch_size = len(batch)\n",
    "    hidden_size = batch[0]['inputs_embeds'].shape[1]\n",
    "    \n",
    "    # Initialize padded tensors\n",
    "    padded_input_embeds = torch.rand((batch_size, max_len, hidden_size), dtype=DTYPE)\n",
    "    padded_attention_mask = torch.zeros((batch_size, max_len))\n",
    "    padded_labels = torch.full((batch_size, max_len), fill_value=-100)\n",
    "    \n",
    "    # Fill padded tensors with actual values\n",
    "    for i, item in enumerate(batch):\n",
    "        seq_len = item['inputs_embeds'].shape[0]\n",
    "        padded_input_embeds[i, :seq_len] = item['inputs_embeds']\n",
    "        padded_attention_mask[i, :seq_len] = item['attention_mask']\n",
    "        padded_labels[i, :seq_len] = item['labels']\n",
    "        \n",
    "    return {\n",
    "        'inputs_embeds': padded_input_embeds,\n",
    "        'attention_mask': padded_attention_mask,\n",
    "        'labels': padded_labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = copy.deepcopy(base_model.config)\n",
    "config.num_hidden_layers = 1\n",
    "config._attn_implementation_autoset = False\n",
    "config._attn_implementation = \"eager\"\n",
    "config.rms_norm_eps = 1e-5\n",
    "model = transformers.LlamaForCausalLM(config).to(\n",
    "    dtype=DTYPE\n",
    ").to(\n",
    "    device=DEVICE\n",
    ")\n",
    "for original_module, new_module in [\n",
    "    (base_model.model.embed_tokens, model.model.embed_tokens),\n",
    "    (base_model.model.norm, model.model.norm),\n",
    "    (base_model.model.rotary_emb, model.model.rotary_emb),\n",
    "    (base_model.lm_head, model.lm_head),\n",
    "]:\n",
    "    new_module.load_state_dict(original_module.state_dict())\n",
    "for freeze_module in [model.model.embed_tokens, model.model.norm, model.model.rotary_emb, model.lm_head]:\n",
    "    for param in freeze_module.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.model.layers[0].load_state_dict(\n",
    "#    original_layers[0].state_dict()\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add gradient clipping to RMSNorm layers\n",
    "for layer in model.model.layers:\n",
    "    layer.input_layernorm.weight.data.fill_(1.0)\n",
    "    layer.post_attention_layernorm.weight.data.fill_(1.0)\n",
    "\n",
    "for parameter in model.parameters():\n",
    "    if parameter.requires_grad:\n",
    "        parameter.register_hook(lambda grad: torch.clamp(grad, -1.0, 1.0))\n",
    "\n",
    "def custom_rmsnorm_forward(self, hidden_states):\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "    # Add value clipping here\n",
    "    hidden_states = torch.clamp(hidden_states, -10.0, 10.0)\n",
    "    return self.weight * hidden_states\n",
    "\n",
    "# Monkey patch the RMSNorm forward method\n",
    "for layer in model.model.layers:\n",
    "    layer.input_layernorm.forward = lambda x: custom_rmsnorm_forward(layer.input_layernorm, x)\n",
    "    layer.post_attention_layernorm.forward = lambda x: custom_rmsnorm_forward(layer.post_attention_layernorm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaAttention\n",
    "\n",
    "\n",
    "def custom_attention_forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        position_embeddings,\n",
    "        attention_mask,\n",
    "        past_key_value,\n",
    "        cache_position,\n",
    "        **kwargs,\n",
    "    ):\n",
    "    # Call the original forward\n",
    "    outputs = LlamaAttention.forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        position_embeddings,\n",
    "        attention_mask,\n",
    "        past_key_value,\n",
    "        cache_position,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # If outputs is a tuple (for causal attention), get the first element\n",
    "    if isinstance(outputs, tuple):\n",
    "        attn_output = outputs[0]\n",
    "    else:\n",
    "        attn_output = outputs\n",
    "        \n",
    "    # Clip the attention output\n",
    "    attn_output = torch.clamp(attn_output, -10.0, 10.0)\n",
    "    \n",
    "    # Return the clipped output in the same format as the original\n",
    "    if isinstance(outputs, tuple):\n",
    "        return (attn_output,) + outputs[1:]\n",
    "    return attn_output\n",
    "\n",
    "def build_custom_attention_forward(layer):\n",
    "    return lambda *args, **kwargs: custom_attention_forward(layer, *args, **kwargs)\n",
    "\n",
    "# Monkey patch the attention forward method\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.forward = build_custom_attention_forward(layer.self_attn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add gradient clipping to attention parameters\n",
    "for layer in model.model.layers:\n",
    "    for name, param in layer.self_attn.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            param.register_hook(lambda grad: torch.clamp(grad, -0.1, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 4\n",
    "#GRADIENT_ACCUMULATION_STEPS = 16\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "LR = 1e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_EPOCHS = 1\n",
    "SAVE_STEPS = 2000\n",
    "WARMUP_STEPS = 500\n",
    "MAX_GRAD_NORN = 0.05\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PrintGradNormCallback(transformers.TrainerCallback):\n",
    "#    def on_pre_optimizer_step(self, args, state, control, model, **kwargs):\n",
    "#        for name, param in model.named_parameters():\n",
    "#            print(name)\n",
    "#            if param.grad is not None:\n",
    "#                param_norm = param.grad.detach().data.norm(2)\n",
    "#                print(name, param_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7309, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2817, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2964, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1835' max='1835' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1835/1835 07:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>13.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.255400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>12.970600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>13.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>14.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>13.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>13.472800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>13.838900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>12.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>13.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12.637200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>12.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>13.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>13.437500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>13.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>13.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>12.818800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>13.308700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>13.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>14.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>14.562200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>13.782100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>13.607800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>12.978800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>13.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>13.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>14.269100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>13.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>13.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>13.719000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>12.916800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>14.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>13.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>13.497300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>13.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>13.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>13.508400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>12.910200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>13.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>13.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>13.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>13.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>12.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>14.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>14.393000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>13.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>13.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>13.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>13.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>13.328600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>13.922500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>13.234400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>13.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>14.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>13.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>13.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>13.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>13.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>13.952800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>13.103800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>12.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>13.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>13.675800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>13.391700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>13.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>12.999800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>12.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>12.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>14.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>14.324200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>12.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>13.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>13.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>13.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>12.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>13.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>13.132800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>14.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>13.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>13.772600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>13.843700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>12.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>13.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>12.877300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>12.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>13.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>13.406100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>13.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>13.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>13.311000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>13.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>12.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>13.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>14.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>13.110700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>13.194800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>13.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>12.456400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>13.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>13.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>12.975800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>13.985200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>13.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>13.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>12.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>13.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>13.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>13.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>13.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>12.837900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>13.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>12.715800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>13.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>12.466100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>12.577600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>13.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>13.424000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>12.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>13.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>13.275900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>12.729300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>13.458300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>13.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>13.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>13.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>12.846300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>13.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>13.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>12.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>12.407700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>12.584800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>13.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>13.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>13.150800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>12.906200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>13.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>12.996700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>12.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>12.468100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>13.227800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>12.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>13.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>14.501100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>11.998700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>12.721300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>13.047400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>12.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>13.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>12.587800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>12.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>12.870400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>12.545200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>12.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>13.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>13.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>12.614000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>12.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>13.118500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>12.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>12.319700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>12.235600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>13.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>13.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>13.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>11.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>12.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>13.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>12.715700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>12.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>12.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>12.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>13.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>12.134900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>12.145100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>12.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>12.695300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>12.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>11.283600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>11.474900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>11.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>12.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>12.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>12.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>11.974100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>11.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>12.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>12.323900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>11.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>12.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>11.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>11.381300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>11.544000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>12.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>12.419400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>11.707000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>11.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>13.117900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>11.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>12.270000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>12.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>10.948300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>11.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>12.472900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>11.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>11.710200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>10.982500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>12.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>10.528100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>11.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>12.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>12.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>11.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>11.927600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>11.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>10.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>11.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>11.225000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>11.287900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>12.077600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>12.876000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>11.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>12.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>11.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>11.638700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>10.575900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>11.571400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>10.585800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>11.111700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>11.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>11.068600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>11.986800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>11.903000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>12.356100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>12.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>10.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>11.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>11.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>10.992500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>11.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>10.501700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>10.702400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>10.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>10.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>11.309300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>10.525600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>10.962500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>10.833200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>10.125400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>10.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>10.488700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>10.602800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>10.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>11.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>10.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>10.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>11.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>12.438800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>10.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>11.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>11.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>10.476100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>10.141900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>10.658200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>11.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>10.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>10.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>11.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>9.761200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>10.894600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>13.211000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>10.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>10.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>11.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>11.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>10.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>9.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>10.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>10.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>10.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>10.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>11.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>10.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>9.878200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>13.159000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>10.127400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>10.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>10.151900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>10.146800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>10.967100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>11.789500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>9.656000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>10.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>9.946300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>10.149800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>9.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>10.352900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>10.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>10.369500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>9.651200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>10.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>10.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>11.293200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>10.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>9.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>10.153500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>9.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>10.813200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>8.931500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>9.885800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>10.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>10.176300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>9.935600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>10.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>10.184300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>10.422700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>10.616800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>9.593500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>10.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>10.602300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>9.694900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>10.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>10.345400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>10.735200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>10.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>10.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>10.591800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>10.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>10.587300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>10.209200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>11.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>9.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>10.607100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>10.639100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>10.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>9.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>10.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>9.632500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>9.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>10.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>10.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>10.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>10.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>10.469500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>9.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>9.848000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>9.921900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>9.484300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>11.337500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>9.907800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>9.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>9.611700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>9.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>10.351600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>9.795500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>9.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>9.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>10.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>10.389400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>9.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>9.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>9.906000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>9.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>10.102700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>9.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>9.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>10.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>9.468900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>8.679200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>10.482200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>9.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>9.520300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>9.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>9.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>10.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>377</td>\n",
       "      <td>10.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>378</td>\n",
       "      <td>9.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>379</td>\n",
       "      <td>8.614200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>9.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>381</td>\n",
       "      <td>10.468400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>382</td>\n",
       "      <td>9.786400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>383</td>\n",
       "      <td>10.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>9.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385</td>\n",
       "      <td>9.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>386</td>\n",
       "      <td>9.091800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>387</td>\n",
       "      <td>9.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>10.364900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>389</td>\n",
       "      <td>10.463700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>11.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>391</td>\n",
       "      <td>10.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>392</td>\n",
       "      <td>9.947400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>393</td>\n",
       "      <td>9.406600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>394</td>\n",
       "      <td>10.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395</td>\n",
       "      <td>10.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>396</td>\n",
       "      <td>10.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>397</td>\n",
       "      <td>9.957800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>398</td>\n",
       "      <td>10.364500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399</td>\n",
       "      <td>9.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>10.251500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>401</td>\n",
       "      <td>9.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>402</td>\n",
       "      <td>8.930200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>403</td>\n",
       "      <td>11.199400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>404</td>\n",
       "      <td>11.567000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>9.529700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>406</td>\n",
       "      <td>9.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>407</td>\n",
       "      <td>9.064600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>408</td>\n",
       "      <td>9.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409</td>\n",
       "      <td>10.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>10.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>411</td>\n",
       "      <td>9.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>412</td>\n",
       "      <td>10.344900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>413</td>\n",
       "      <td>8.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>414</td>\n",
       "      <td>9.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415</td>\n",
       "      <td>9.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>416</td>\n",
       "      <td>9.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>417</td>\n",
       "      <td>9.610300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418</td>\n",
       "      <td>9.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>419</td>\n",
       "      <td>10.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>9.916300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>421</td>\n",
       "      <td>9.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>422</td>\n",
       "      <td>9.129400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>423</td>\n",
       "      <td>10.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>424</td>\n",
       "      <td>9.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>9.494300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>426</td>\n",
       "      <td>10.517800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>427</td>\n",
       "      <td>9.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>428</td>\n",
       "      <td>10.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>9.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>9.742500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>10.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>9.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>10.181900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>9.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>9.170400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>436</td>\n",
       "      <td>10.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>437</td>\n",
       "      <td>10.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>9.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td>9.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>10.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>441</td>\n",
       "      <td>9.535000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>442</td>\n",
       "      <td>9.623700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>9.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>444</td>\n",
       "      <td>9.606700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445</td>\n",
       "      <td>9.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>446</td>\n",
       "      <td>9.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>10.811900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>448</td>\n",
       "      <td>9.437600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>8.796400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>9.943800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>451</td>\n",
       "      <td>9.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>452</td>\n",
       "      <td>8.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>453</td>\n",
       "      <td>9.908900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>454</td>\n",
       "      <td>9.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455</td>\n",
       "      <td>9.272400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>456</td>\n",
       "      <td>9.626500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>457</td>\n",
       "      <td>9.274500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>458</td>\n",
       "      <td>9.961000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>459</td>\n",
       "      <td>8.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>9.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>461</td>\n",
       "      <td>9.641300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>9.859500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>463</td>\n",
       "      <td>8.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>9.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465</td>\n",
       "      <td>9.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>466</td>\n",
       "      <td>9.301600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>467</td>\n",
       "      <td>9.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>468</td>\n",
       "      <td>9.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>469</td>\n",
       "      <td>10.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>9.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>471</td>\n",
       "      <td>9.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>472</td>\n",
       "      <td>8.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>473</td>\n",
       "      <td>9.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>474</td>\n",
       "      <td>9.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>9.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>476</td>\n",
       "      <td>10.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>477</td>\n",
       "      <td>10.879000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>478</td>\n",
       "      <td>8.808300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>479</td>\n",
       "      <td>9.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>9.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>481</td>\n",
       "      <td>8.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>482</td>\n",
       "      <td>8.931000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>483</td>\n",
       "      <td>9.122200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>484</td>\n",
       "      <td>8.996300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485</td>\n",
       "      <td>8.962200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>486</td>\n",
       "      <td>10.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>487</td>\n",
       "      <td>9.381400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>488</td>\n",
       "      <td>11.146900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>489</td>\n",
       "      <td>8.853400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>10.611100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>491</td>\n",
       "      <td>9.956400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>492</td>\n",
       "      <td>9.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>493</td>\n",
       "      <td>9.551700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>494</td>\n",
       "      <td>10.375600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495</td>\n",
       "      <td>10.379400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>496</td>\n",
       "      <td>8.842200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>497</td>\n",
       "      <td>10.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>498</td>\n",
       "      <td>8.491500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>499</td>\n",
       "      <td>8.482000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>9.795200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>501</td>\n",
       "      <td>10.063500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>502</td>\n",
       "      <td>8.598200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>503</td>\n",
       "      <td>9.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>504</td>\n",
       "      <td>9.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505</td>\n",
       "      <td>9.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>506</td>\n",
       "      <td>9.853700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>507</td>\n",
       "      <td>9.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>508</td>\n",
       "      <td>10.757700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>509</td>\n",
       "      <td>9.105600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>9.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>511</td>\n",
       "      <td>9.747200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>9.077000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>10.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>9.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515</td>\n",
       "      <td>9.384100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>516</td>\n",
       "      <td>9.591600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>517</td>\n",
       "      <td>8.360100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>9.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>519</td>\n",
       "      <td>9.987400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>9.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>521</td>\n",
       "      <td>9.400700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>522</td>\n",
       "      <td>8.788700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>9.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>524</td>\n",
       "      <td>9.590200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>10.477600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>526</td>\n",
       "      <td>7.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>9.696300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>528</td>\n",
       "      <td>10.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>529</td>\n",
       "      <td>10.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>9.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>9.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>8.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>533</td>\n",
       "      <td>8.574700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>9.448400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535</td>\n",
       "      <td>9.199700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>536</td>\n",
       "      <td>9.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>537</td>\n",
       "      <td>9.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>538</td>\n",
       "      <td>8.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>539</td>\n",
       "      <td>10.201700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>9.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>541</td>\n",
       "      <td>9.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>542</td>\n",
       "      <td>10.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>543</td>\n",
       "      <td>9.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>544</td>\n",
       "      <td>8.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545</td>\n",
       "      <td>8.164200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>546</td>\n",
       "      <td>9.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>547</td>\n",
       "      <td>8.790100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>548</td>\n",
       "      <td>8.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>549</td>\n",
       "      <td>9.520600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>9.249500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>551</td>\n",
       "      <td>9.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>552</td>\n",
       "      <td>9.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>553</td>\n",
       "      <td>9.226100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>554</td>\n",
       "      <td>9.460200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555</td>\n",
       "      <td>9.387800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>556</td>\n",
       "      <td>9.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>557</td>\n",
       "      <td>9.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>558</td>\n",
       "      <td>8.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>559</td>\n",
       "      <td>8.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>10.978200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>561</td>\n",
       "      <td>9.637300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>562</td>\n",
       "      <td>9.982200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>563</td>\n",
       "      <td>8.753900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>564</td>\n",
       "      <td>9.017300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565</td>\n",
       "      <td>8.758900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>566</td>\n",
       "      <td>9.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>567</td>\n",
       "      <td>8.525900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>9.652600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>569</td>\n",
       "      <td>8.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>8.445200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>571</td>\n",
       "      <td>9.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>572</td>\n",
       "      <td>9.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>573</td>\n",
       "      <td>8.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>574</td>\n",
       "      <td>9.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>9.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>576</td>\n",
       "      <td>9.138600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>577</td>\n",
       "      <td>9.052600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>578</td>\n",
       "      <td>10.607600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>579</td>\n",
       "      <td>9.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>9.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>581</td>\n",
       "      <td>9.884700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>582</td>\n",
       "      <td>10.093200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>583</td>\n",
       "      <td>9.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>584</td>\n",
       "      <td>9.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585</td>\n",
       "      <td>9.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>586</td>\n",
       "      <td>9.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>587</td>\n",
       "      <td>8.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>588</td>\n",
       "      <td>9.283000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>589</td>\n",
       "      <td>9.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>10.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>591</td>\n",
       "      <td>9.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>592</td>\n",
       "      <td>8.500800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>593</td>\n",
       "      <td>9.731400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>594</td>\n",
       "      <td>10.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595</td>\n",
       "      <td>9.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>596</td>\n",
       "      <td>7.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>597</td>\n",
       "      <td>9.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>598</td>\n",
       "      <td>9.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599</td>\n",
       "      <td>8.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>9.061600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>601</td>\n",
       "      <td>8.311300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>602</td>\n",
       "      <td>9.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>603</td>\n",
       "      <td>9.441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>604</td>\n",
       "      <td>9.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605</td>\n",
       "      <td>8.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>606</td>\n",
       "      <td>10.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>607</td>\n",
       "      <td>9.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>608</td>\n",
       "      <td>8.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>609</td>\n",
       "      <td>8.745200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>7.716600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>8.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>612</td>\n",
       "      <td>9.059100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>613</td>\n",
       "      <td>9.458000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>614</td>\n",
       "      <td>8.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615</td>\n",
       "      <td>9.150200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>8.888400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>617</td>\n",
       "      <td>8.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>618</td>\n",
       "      <td>9.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>619</td>\n",
       "      <td>9.122500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>9.694100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>621</td>\n",
       "      <td>8.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>622</td>\n",
       "      <td>9.028000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>623</td>\n",
       "      <td>9.119500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>624</td>\n",
       "      <td>7.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>9.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>626</td>\n",
       "      <td>9.492100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>627</td>\n",
       "      <td>8.787000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>628</td>\n",
       "      <td>9.106100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>629</td>\n",
       "      <td>9.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>8.529200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>631</td>\n",
       "      <td>9.572500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>632</td>\n",
       "      <td>9.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>633</td>\n",
       "      <td>9.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>634</td>\n",
       "      <td>9.447800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635</td>\n",
       "      <td>9.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>636</td>\n",
       "      <td>10.073800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>637</td>\n",
       "      <td>9.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638</td>\n",
       "      <td>9.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>639</td>\n",
       "      <td>9.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>9.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>641</td>\n",
       "      <td>9.149500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>642</td>\n",
       "      <td>9.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>643</td>\n",
       "      <td>9.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>644</td>\n",
       "      <td>8.882300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645</td>\n",
       "      <td>9.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>646</td>\n",
       "      <td>10.318900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>8.620800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>648</td>\n",
       "      <td>9.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>649</td>\n",
       "      <td>9.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>8.051700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>651</td>\n",
       "      <td>8.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>652</td>\n",
       "      <td>9.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>653</td>\n",
       "      <td>8.823000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>654</td>\n",
       "      <td>8.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655</td>\n",
       "      <td>8.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>656</td>\n",
       "      <td>8.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>657</td>\n",
       "      <td>9.526100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>658</td>\n",
       "      <td>9.005300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>659</td>\n",
       "      <td>8.858200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>7.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>661</td>\n",
       "      <td>8.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>662</td>\n",
       "      <td>8.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>663</td>\n",
       "      <td>8.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>664</td>\n",
       "      <td>9.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665</td>\n",
       "      <td>8.539100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>666</td>\n",
       "      <td>8.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>667</td>\n",
       "      <td>9.471600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>668</td>\n",
       "      <td>9.525800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>669</td>\n",
       "      <td>10.080900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>9.072700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>671</td>\n",
       "      <td>8.382700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>672</td>\n",
       "      <td>9.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>673</td>\n",
       "      <td>8.717200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>674</td>\n",
       "      <td>9.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>8.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>676</td>\n",
       "      <td>9.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>677</td>\n",
       "      <td>8.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>678</td>\n",
       "      <td>10.551400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>679</td>\n",
       "      <td>8.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>8.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>681</td>\n",
       "      <td>8.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>682</td>\n",
       "      <td>8.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>683</td>\n",
       "      <td>11.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>8.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685</td>\n",
       "      <td>9.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>686</td>\n",
       "      <td>9.609600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>687</td>\n",
       "      <td>8.535500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>688</td>\n",
       "      <td>9.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>689</td>\n",
       "      <td>9.445400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>8.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>691</td>\n",
       "      <td>8.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>9.715300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693</td>\n",
       "      <td>8.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>694</td>\n",
       "      <td>9.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695</td>\n",
       "      <td>8.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>696</td>\n",
       "      <td>8.479400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>697</td>\n",
       "      <td>8.810600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>698</td>\n",
       "      <td>9.890100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>699</td>\n",
       "      <td>8.951800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>9.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>701</td>\n",
       "      <td>8.074300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>702</td>\n",
       "      <td>9.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>703</td>\n",
       "      <td>10.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>704</td>\n",
       "      <td>8.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705</td>\n",
       "      <td>8.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>706</td>\n",
       "      <td>9.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>707</td>\n",
       "      <td>8.058800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>708</td>\n",
       "      <td>9.239400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>709</td>\n",
       "      <td>8.726700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>8.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>711</td>\n",
       "      <td>10.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>712</td>\n",
       "      <td>8.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>9.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>7.761300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>8.951600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>8.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>8.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>9.123200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>8.905500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>9.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>721</td>\n",
       "      <td>9.882100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>722</td>\n",
       "      <td>8.522200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>723</td>\n",
       "      <td>8.641500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>724</td>\n",
       "      <td>8.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>8.217300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726</td>\n",
       "      <td>9.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>727</td>\n",
       "      <td>7.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>728</td>\n",
       "      <td>9.159100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>729</td>\n",
       "      <td>9.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>9.682400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>731</td>\n",
       "      <td>9.894800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>732</td>\n",
       "      <td>8.174300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>733</td>\n",
       "      <td>8.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>734</td>\n",
       "      <td>8.924600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735</td>\n",
       "      <td>9.283800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>736</td>\n",
       "      <td>8.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>737</td>\n",
       "      <td>9.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>738</td>\n",
       "      <td>9.367700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>739</td>\n",
       "      <td>10.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>8.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>741</td>\n",
       "      <td>8.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>742</td>\n",
       "      <td>9.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>743</td>\n",
       "      <td>8.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>744</td>\n",
       "      <td>8.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745</td>\n",
       "      <td>8.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>746</td>\n",
       "      <td>9.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>747</td>\n",
       "      <td>8.767400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>748</td>\n",
       "      <td>9.631700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>749</td>\n",
       "      <td>8.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>8.391100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>751</td>\n",
       "      <td>10.603500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>752</td>\n",
       "      <td>8.683100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>753</td>\n",
       "      <td>9.610100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>754</td>\n",
       "      <td>11.023800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755</td>\n",
       "      <td>8.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>756</td>\n",
       "      <td>9.148800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>757</td>\n",
       "      <td>9.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>758</td>\n",
       "      <td>9.129800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>759</td>\n",
       "      <td>8.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>8.728400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>9.491100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>762</td>\n",
       "      <td>8.785900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>763</td>\n",
       "      <td>9.065500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>764</td>\n",
       "      <td>9.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765</td>\n",
       "      <td>9.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>766</td>\n",
       "      <td>9.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>767</td>\n",
       "      <td>9.004100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>8.227400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769</td>\n",
       "      <td>10.853900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>9.224200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>771</td>\n",
       "      <td>9.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>772</td>\n",
       "      <td>9.175500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>773</td>\n",
       "      <td>9.857600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>774</td>\n",
       "      <td>9.432300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>9.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>776</td>\n",
       "      <td>8.514100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>777</td>\n",
       "      <td>8.996100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>778</td>\n",
       "      <td>7.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>779</td>\n",
       "      <td>9.038900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>9.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>781</td>\n",
       "      <td>8.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>782</td>\n",
       "      <td>10.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>783</td>\n",
       "      <td>9.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>784</td>\n",
       "      <td>9.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785</td>\n",
       "      <td>9.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>786</td>\n",
       "      <td>8.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>787</td>\n",
       "      <td>10.017900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>788</td>\n",
       "      <td>9.386300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>789</td>\n",
       "      <td>9.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>8.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>791</td>\n",
       "      <td>8.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>792</td>\n",
       "      <td>8.796600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>793</td>\n",
       "      <td>8.367300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>794</td>\n",
       "      <td>8.198800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795</td>\n",
       "      <td>9.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>796</td>\n",
       "      <td>8.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>797</td>\n",
       "      <td>8.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>798</td>\n",
       "      <td>8.403700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>799</td>\n",
       "      <td>8.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>10.382300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>801</td>\n",
       "      <td>9.094100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>802</td>\n",
       "      <td>8.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>803</td>\n",
       "      <td>7.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>804</td>\n",
       "      <td>9.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805</td>\n",
       "      <td>8.989800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>806</td>\n",
       "      <td>8.489000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>807</td>\n",
       "      <td>9.484100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>808</td>\n",
       "      <td>8.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>809</td>\n",
       "      <td>8.829500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>9.272300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>811</td>\n",
       "      <td>8.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>812</td>\n",
       "      <td>10.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>813</td>\n",
       "      <td>9.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>814</td>\n",
       "      <td>8.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815</td>\n",
       "      <td>9.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>816</td>\n",
       "      <td>9.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>817</td>\n",
       "      <td>10.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>818</td>\n",
       "      <td>8.529000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>819</td>\n",
       "      <td>8.831700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>9.065300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>9.002700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>822</td>\n",
       "      <td>9.182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>823</td>\n",
       "      <td>8.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>824</td>\n",
       "      <td>7.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>8.097400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>826</td>\n",
       "      <td>9.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>827</td>\n",
       "      <td>8.843200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>828</td>\n",
       "      <td>8.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>829</td>\n",
       "      <td>7.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>7.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>831</td>\n",
       "      <td>9.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>832</td>\n",
       "      <td>9.077700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>833</td>\n",
       "      <td>8.052900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>834</td>\n",
       "      <td>9.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835</td>\n",
       "      <td>9.766700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>836</td>\n",
       "      <td>9.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837</td>\n",
       "      <td>10.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>838</td>\n",
       "      <td>9.742200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>839</td>\n",
       "      <td>9.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>8.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>841</td>\n",
       "      <td>9.434500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>842</td>\n",
       "      <td>8.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>843</td>\n",
       "      <td>9.613700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>844</td>\n",
       "      <td>8.585900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>9.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>846</td>\n",
       "      <td>9.609900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>847</td>\n",
       "      <td>8.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>848</td>\n",
       "      <td>8.166300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>849</td>\n",
       "      <td>9.555300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>8.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851</td>\n",
       "      <td>7.942500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>8.321400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>853</td>\n",
       "      <td>8.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>854</td>\n",
       "      <td>9.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855</td>\n",
       "      <td>7.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856</td>\n",
       "      <td>8.897100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>857</td>\n",
       "      <td>9.071500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858</td>\n",
       "      <td>8.817700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>859</td>\n",
       "      <td>9.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>9.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>861</td>\n",
       "      <td>9.932900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>862</td>\n",
       "      <td>9.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>863</td>\n",
       "      <td>8.343800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>864</td>\n",
       "      <td>9.232100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865</td>\n",
       "      <td>9.788000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>866</td>\n",
       "      <td>9.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>867</td>\n",
       "      <td>9.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>8.681500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>869</td>\n",
       "      <td>9.797300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>8.040300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>871</td>\n",
       "      <td>8.242100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>872</td>\n",
       "      <td>8.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>873</td>\n",
       "      <td>8.830500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>874</td>\n",
       "      <td>8.272100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>8.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>876</td>\n",
       "      <td>10.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>877</td>\n",
       "      <td>9.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>878</td>\n",
       "      <td>8.923800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>879</td>\n",
       "      <td>8.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>8.667600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>881</td>\n",
       "      <td>8.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>882</td>\n",
       "      <td>9.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>883</td>\n",
       "      <td>9.578400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>884</td>\n",
       "      <td>8.660900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885</td>\n",
       "      <td>8.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>8.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>8.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>8.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>8.728200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>8.659400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>891</td>\n",
       "      <td>9.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>892</td>\n",
       "      <td>9.196700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>893</td>\n",
       "      <td>9.694300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>894</td>\n",
       "      <td>7.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895</td>\n",
       "      <td>8.596500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>8.061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>897</td>\n",
       "      <td>9.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>898</td>\n",
       "      <td>8.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>899</td>\n",
       "      <td>8.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>9.732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>901</td>\n",
       "      <td>8.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>902</td>\n",
       "      <td>9.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>903</td>\n",
       "      <td>8.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>904</td>\n",
       "      <td>8.535400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905</td>\n",
       "      <td>9.125700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>906</td>\n",
       "      <td>9.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>907</td>\n",
       "      <td>9.045900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>908</td>\n",
       "      <td>9.320500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>909</td>\n",
       "      <td>8.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>8.613600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>8.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>912</td>\n",
       "      <td>8.865800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>8.133500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>914</td>\n",
       "      <td>8.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915</td>\n",
       "      <td>8.314300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>916</td>\n",
       "      <td>9.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>917</td>\n",
       "      <td>11.210700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>918</td>\n",
       "      <td>9.481500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>919</td>\n",
       "      <td>8.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>8.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921</td>\n",
       "      <td>11.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>922</td>\n",
       "      <td>8.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>923</td>\n",
       "      <td>8.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>9.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>7.584200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>926</td>\n",
       "      <td>9.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>927</td>\n",
       "      <td>8.182800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>928</td>\n",
       "      <td>8.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>929</td>\n",
       "      <td>7.946000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>9.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>931</td>\n",
       "      <td>8.934100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>932</td>\n",
       "      <td>8.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>933</td>\n",
       "      <td>9.114400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>934</td>\n",
       "      <td>8.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935</td>\n",
       "      <td>8.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>936</td>\n",
       "      <td>8.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>937</td>\n",
       "      <td>8.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>938</td>\n",
       "      <td>8.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>939</td>\n",
       "      <td>8.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>8.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>941</td>\n",
       "      <td>9.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>942</td>\n",
       "      <td>8.859400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>943</td>\n",
       "      <td>9.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>944</td>\n",
       "      <td>8.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945</td>\n",
       "      <td>9.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>946</td>\n",
       "      <td>8.111900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>947</td>\n",
       "      <td>8.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>948</td>\n",
       "      <td>7.417300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>949</td>\n",
       "      <td>8.627600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>9.187800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>951</td>\n",
       "      <td>9.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>952</td>\n",
       "      <td>8.737300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>953</td>\n",
       "      <td>8.929900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>954</td>\n",
       "      <td>9.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955</td>\n",
       "      <td>8.380200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>956</td>\n",
       "      <td>9.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>957</td>\n",
       "      <td>7.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>958</td>\n",
       "      <td>8.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>959</td>\n",
       "      <td>8.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>8.644300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>961</td>\n",
       "      <td>8.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>962</td>\n",
       "      <td>9.792400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>963</td>\n",
       "      <td>9.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>964</td>\n",
       "      <td>9.898300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965</td>\n",
       "      <td>8.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>966</td>\n",
       "      <td>9.856400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>967</td>\n",
       "      <td>9.069900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>968</td>\n",
       "      <td>9.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>969</td>\n",
       "      <td>8.761400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>8.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>971</td>\n",
       "      <td>8.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>972</td>\n",
       "      <td>9.490500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>973</td>\n",
       "      <td>8.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>974</td>\n",
       "      <td>8.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>8.601700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>976</td>\n",
       "      <td>8.288400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>977</td>\n",
       "      <td>9.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>978</td>\n",
       "      <td>8.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>979</td>\n",
       "      <td>8.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>8.637600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>981</td>\n",
       "      <td>8.584100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>982</td>\n",
       "      <td>9.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>983</td>\n",
       "      <td>8.671300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>984</td>\n",
       "      <td>8.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985</td>\n",
       "      <td>8.847200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>986</td>\n",
       "      <td>8.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>987</td>\n",
       "      <td>8.788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>988</td>\n",
       "      <td>8.864800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>989</td>\n",
       "      <td>9.533100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>8.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>991</td>\n",
       "      <td>9.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>992</td>\n",
       "      <td>8.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>993</td>\n",
       "      <td>9.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>994</td>\n",
       "      <td>7.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995</td>\n",
       "      <td>9.084600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>996</td>\n",
       "      <td>9.289900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>997</td>\n",
       "      <td>10.489600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>998</td>\n",
       "      <td>8.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>999</td>\n",
       "      <td>8.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>8.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1001</td>\n",
       "      <td>9.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1002</td>\n",
       "      <td>9.077100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1003</td>\n",
       "      <td>8.918500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1004</td>\n",
       "      <td>9.432900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005</td>\n",
       "      <td>7.761600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1006</td>\n",
       "      <td>8.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1007</td>\n",
       "      <td>9.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1008</td>\n",
       "      <td>10.490800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1009</td>\n",
       "      <td>8.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>8.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1011</td>\n",
       "      <td>9.521000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1012</td>\n",
       "      <td>7.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1013</td>\n",
       "      <td>8.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1014</td>\n",
       "      <td>8.216900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015</td>\n",
       "      <td>8.093900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1016</td>\n",
       "      <td>8.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>9.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1018</td>\n",
       "      <td>10.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1019</td>\n",
       "      <td>8.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>10.774700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1021</td>\n",
       "      <td>8.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1022</td>\n",
       "      <td>8.925100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1023</td>\n",
       "      <td>8.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1024</td>\n",
       "      <td>8.426300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>8.904900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1026</td>\n",
       "      <td>8.702500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1027</td>\n",
       "      <td>8.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1028</td>\n",
       "      <td>8.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1029</td>\n",
       "      <td>8.624400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>9.525300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1031</td>\n",
       "      <td>8.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1032</td>\n",
       "      <td>8.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1033</td>\n",
       "      <td>8.964600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1034</td>\n",
       "      <td>8.488800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035</td>\n",
       "      <td>7.818300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036</td>\n",
       "      <td>8.189400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1037</td>\n",
       "      <td>9.501200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1038</td>\n",
       "      <td>8.924400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1039</td>\n",
       "      <td>8.314200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>9.119200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1041</td>\n",
       "      <td>8.552800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1042</td>\n",
       "      <td>9.009300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>8.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>10.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045</td>\n",
       "      <td>8.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1046</td>\n",
       "      <td>9.633700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>6.988900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1048</td>\n",
       "      <td>8.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>10.456000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>8.726800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1051</td>\n",
       "      <td>8.801000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1052</td>\n",
       "      <td>9.109300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1053</td>\n",
       "      <td>6.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1054</td>\n",
       "      <td>8.652100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>8.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1056</td>\n",
       "      <td>9.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1057</td>\n",
       "      <td>8.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1058</td>\n",
       "      <td>8.938900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1059</td>\n",
       "      <td>9.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>8.105700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1061</td>\n",
       "      <td>8.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1062</td>\n",
       "      <td>8.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1063</td>\n",
       "      <td>8.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1064</td>\n",
       "      <td>9.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065</td>\n",
       "      <td>9.348000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1066</td>\n",
       "      <td>8.723900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1067</td>\n",
       "      <td>9.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1068</td>\n",
       "      <td>8.533600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1069</td>\n",
       "      <td>8.873200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>10.832500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1071</td>\n",
       "      <td>11.623300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1072</td>\n",
       "      <td>8.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1073</td>\n",
       "      <td>7.511800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1074</td>\n",
       "      <td>8.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>8.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1076</td>\n",
       "      <td>8.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1077</td>\n",
       "      <td>8.851000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>9.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1079</td>\n",
       "      <td>9.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>8.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1081</td>\n",
       "      <td>11.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1082</td>\n",
       "      <td>8.836000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1083</td>\n",
       "      <td>8.616100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1084</td>\n",
       "      <td>9.154200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085</td>\n",
       "      <td>9.002000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1086</td>\n",
       "      <td>8.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1087</td>\n",
       "      <td>8.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1088</td>\n",
       "      <td>9.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1089</td>\n",
       "      <td>8.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>9.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1091</td>\n",
       "      <td>8.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1092</td>\n",
       "      <td>9.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1093</td>\n",
       "      <td>8.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1094</td>\n",
       "      <td>8.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095</td>\n",
       "      <td>9.946100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1096</td>\n",
       "      <td>8.146000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1097</td>\n",
       "      <td>8.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1098</td>\n",
       "      <td>7.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1099</td>\n",
       "      <td>7.527200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>9.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1101</td>\n",
       "      <td>8.670700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1102</td>\n",
       "      <td>9.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1103</td>\n",
       "      <td>9.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1104</td>\n",
       "      <td>11.715500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105</td>\n",
       "      <td>9.993400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1106</td>\n",
       "      <td>8.330500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1107</td>\n",
       "      <td>8.655300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1108</td>\n",
       "      <td>8.127900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1109</td>\n",
       "      <td>8.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>8.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1111</td>\n",
       "      <td>8.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1112</td>\n",
       "      <td>8.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1113</td>\n",
       "      <td>8.614700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1114</td>\n",
       "      <td>9.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115</td>\n",
       "      <td>8.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1116</td>\n",
       "      <td>7.902300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1117</td>\n",
       "      <td>8.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1118</td>\n",
       "      <td>8.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1119</td>\n",
       "      <td>10.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>8.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1121</td>\n",
       "      <td>8.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1122</td>\n",
       "      <td>9.336900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1123</td>\n",
       "      <td>8.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1124</td>\n",
       "      <td>8.554000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>9.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1126</td>\n",
       "      <td>8.203300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1127</td>\n",
       "      <td>8.615800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1128</td>\n",
       "      <td>7.902100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1129</td>\n",
       "      <td>9.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>8.442800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1131</td>\n",
       "      <td>9.000100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1132</td>\n",
       "      <td>8.784000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1133</td>\n",
       "      <td>8.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>9.013900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135</td>\n",
       "      <td>8.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>8.627900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1137</td>\n",
       "      <td>8.726600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1138</td>\n",
       "      <td>8.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1139</td>\n",
       "      <td>8.085200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>8.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1141</td>\n",
       "      <td>8.847400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1142</td>\n",
       "      <td>8.571000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1143</td>\n",
       "      <td>9.172500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1144</td>\n",
       "      <td>8.038100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145</td>\n",
       "      <td>7.683900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>9.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1147</td>\n",
       "      <td>8.682300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1148</td>\n",
       "      <td>8.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1149</td>\n",
       "      <td>9.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>8.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1151</td>\n",
       "      <td>8.464100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1152</td>\n",
       "      <td>8.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1153</td>\n",
       "      <td>8.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1154</td>\n",
       "      <td>8.869800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155</td>\n",
       "      <td>8.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1156</td>\n",
       "      <td>8.768200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1157</td>\n",
       "      <td>8.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1158</td>\n",
       "      <td>8.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1159</td>\n",
       "      <td>7.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>8.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1161</td>\n",
       "      <td>7.943300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1162</td>\n",
       "      <td>8.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1163</td>\n",
       "      <td>8.360200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1164</td>\n",
       "      <td>8.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165</td>\n",
       "      <td>7.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1166</td>\n",
       "      <td>9.052700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1167</td>\n",
       "      <td>7.856100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1168</td>\n",
       "      <td>7.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1169</td>\n",
       "      <td>10.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>8.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1171</td>\n",
       "      <td>7.542600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172</td>\n",
       "      <td>8.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1173</td>\n",
       "      <td>8.474700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174</td>\n",
       "      <td>8.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>9.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1176</td>\n",
       "      <td>8.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1177</td>\n",
       "      <td>8.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1178</td>\n",
       "      <td>8.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1179</td>\n",
       "      <td>8.258600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>9.647300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1181</td>\n",
       "      <td>8.215100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1182</td>\n",
       "      <td>8.644600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1183</td>\n",
       "      <td>8.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1184</td>\n",
       "      <td>9.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185</td>\n",
       "      <td>9.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1186</td>\n",
       "      <td>8.691400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1187</td>\n",
       "      <td>9.331000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1188</td>\n",
       "      <td>8.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189</td>\n",
       "      <td>10.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>8.567200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1191</td>\n",
       "      <td>8.967500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1192</td>\n",
       "      <td>8.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1193</td>\n",
       "      <td>8.482700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1194</td>\n",
       "      <td>9.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195</td>\n",
       "      <td>8.198200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1196</td>\n",
       "      <td>7.777400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1197</td>\n",
       "      <td>8.984700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1198</td>\n",
       "      <td>8.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1199</td>\n",
       "      <td>8.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>8.039500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1201</td>\n",
       "      <td>8.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1202</td>\n",
       "      <td>8.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1203</td>\n",
       "      <td>8.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1204</td>\n",
       "      <td>7.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205</td>\n",
       "      <td>7.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1206</td>\n",
       "      <td>7.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1207</td>\n",
       "      <td>8.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1208</td>\n",
       "      <td>8.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1209</td>\n",
       "      <td>8.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>9.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1211</td>\n",
       "      <td>10.178300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1212</td>\n",
       "      <td>7.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1213</td>\n",
       "      <td>9.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1214</td>\n",
       "      <td>9.793200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215</td>\n",
       "      <td>8.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1216</td>\n",
       "      <td>8.910500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1217</td>\n",
       "      <td>8.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1218</td>\n",
       "      <td>8.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1219</td>\n",
       "      <td>9.421600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>9.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1221</td>\n",
       "      <td>8.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>8.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1223</td>\n",
       "      <td>8.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1224</td>\n",
       "      <td>9.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>7.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1226</td>\n",
       "      <td>9.081800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1227</td>\n",
       "      <td>8.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1228</td>\n",
       "      <td>9.568000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1229</td>\n",
       "      <td>9.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>10.046800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1231</td>\n",
       "      <td>8.372800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>8.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1233</td>\n",
       "      <td>8.164500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1234</td>\n",
       "      <td>8.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235</td>\n",
       "      <td>8.603600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1236</td>\n",
       "      <td>7.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1237</td>\n",
       "      <td>9.285600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1238</td>\n",
       "      <td>7.817300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1239</td>\n",
       "      <td>8.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>9.583600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1241</td>\n",
       "      <td>9.283300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1242</td>\n",
       "      <td>8.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1243</td>\n",
       "      <td>8.654800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1244</td>\n",
       "      <td>8.928900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245</td>\n",
       "      <td>8.543600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1246</td>\n",
       "      <td>8.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1247</td>\n",
       "      <td>8.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1248</td>\n",
       "      <td>8.708600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1249</td>\n",
       "      <td>10.152700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>8.516000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1251</td>\n",
       "      <td>8.725300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1252</td>\n",
       "      <td>7.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1253</td>\n",
       "      <td>8.926600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1254</td>\n",
       "      <td>8.471000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255</td>\n",
       "      <td>8.592800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1256</td>\n",
       "      <td>9.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1257</td>\n",
       "      <td>8.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1258</td>\n",
       "      <td>8.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1259</td>\n",
       "      <td>8.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>8.579500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1261</td>\n",
       "      <td>8.404700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1262</td>\n",
       "      <td>9.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1263</td>\n",
       "      <td>8.557900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1264</td>\n",
       "      <td>8.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265</td>\n",
       "      <td>7.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1266</td>\n",
       "      <td>7.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1267</td>\n",
       "      <td>10.180700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1268</td>\n",
       "      <td>8.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1269</td>\n",
       "      <td>8.589300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>8.421900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1271</td>\n",
       "      <td>8.984900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1272</td>\n",
       "      <td>8.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1273</td>\n",
       "      <td>8.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1274</td>\n",
       "      <td>8.249400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>9.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1276</td>\n",
       "      <td>9.096500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1277</td>\n",
       "      <td>9.005400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1278</td>\n",
       "      <td>7.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1279</td>\n",
       "      <td>8.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>7.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1281</td>\n",
       "      <td>7.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1282</td>\n",
       "      <td>8.824300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1283</td>\n",
       "      <td>8.523800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1284</td>\n",
       "      <td>8.116200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285</td>\n",
       "      <td>8.937700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1286</td>\n",
       "      <td>8.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1287</td>\n",
       "      <td>8.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1288</td>\n",
       "      <td>9.952900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1289</td>\n",
       "      <td>8.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>8.801800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1291</td>\n",
       "      <td>8.750500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1292</td>\n",
       "      <td>9.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1293</td>\n",
       "      <td>7.863400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>9.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295</td>\n",
       "      <td>9.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1296</td>\n",
       "      <td>8.698800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1297</td>\n",
       "      <td>7.616400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1298</td>\n",
       "      <td>9.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1299</td>\n",
       "      <td>8.929600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>8.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1301</td>\n",
       "      <td>9.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1302</td>\n",
       "      <td>8.968900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1303</td>\n",
       "      <td>9.597000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>9.595600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>8.863100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>8.629900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>7.911400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1308</td>\n",
       "      <td>8.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1309</td>\n",
       "      <td>8.576300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>9.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1311</td>\n",
       "      <td>8.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1312</td>\n",
       "      <td>8.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1313</td>\n",
       "      <td>8.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1314</td>\n",
       "      <td>8.848700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315</td>\n",
       "      <td>9.511000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1316</td>\n",
       "      <td>8.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1317</td>\n",
       "      <td>9.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1318</td>\n",
       "      <td>9.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1319</td>\n",
       "      <td>8.027300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>9.549700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1321</td>\n",
       "      <td>10.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1322</td>\n",
       "      <td>8.929000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1323</td>\n",
       "      <td>8.628300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1324</td>\n",
       "      <td>9.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>8.617300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1326</td>\n",
       "      <td>7.987700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1327</td>\n",
       "      <td>8.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1328</td>\n",
       "      <td>7.861500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1329</td>\n",
       "      <td>7.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>7.786900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1331</td>\n",
       "      <td>8.856200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1332</td>\n",
       "      <td>8.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1333</td>\n",
       "      <td>8.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1334</td>\n",
       "      <td>7.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335</td>\n",
       "      <td>8.479900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1336</td>\n",
       "      <td>9.527300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1337</td>\n",
       "      <td>8.648300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1338</td>\n",
       "      <td>8.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1339</td>\n",
       "      <td>8.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>8.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1341</td>\n",
       "      <td>8.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1342</td>\n",
       "      <td>7.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1343</td>\n",
       "      <td>8.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1344</td>\n",
       "      <td>8.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345</td>\n",
       "      <td>8.058400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1346</td>\n",
       "      <td>8.702200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1347</td>\n",
       "      <td>8.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1348</td>\n",
       "      <td>9.477300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1349</td>\n",
       "      <td>8.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>8.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1351</td>\n",
       "      <td>9.254500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1352</td>\n",
       "      <td>8.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1353</td>\n",
       "      <td>8.015200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1354</td>\n",
       "      <td>9.655400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355</td>\n",
       "      <td>8.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1356</td>\n",
       "      <td>9.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1357</td>\n",
       "      <td>9.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1358</td>\n",
       "      <td>8.111000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1359</td>\n",
       "      <td>8.140700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>9.131300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1361</td>\n",
       "      <td>8.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1362</td>\n",
       "      <td>9.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1363</td>\n",
       "      <td>10.618100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1364</td>\n",
       "      <td>8.558800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365</td>\n",
       "      <td>8.517200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1366</td>\n",
       "      <td>8.756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1367</td>\n",
       "      <td>8.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1368</td>\n",
       "      <td>8.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1369</td>\n",
       "      <td>9.653100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>8.981900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1371</td>\n",
       "      <td>9.231400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1372</td>\n",
       "      <td>9.515000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1373</td>\n",
       "      <td>8.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1374</td>\n",
       "      <td>8.485100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>8.383800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1376</td>\n",
       "      <td>7.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1377</td>\n",
       "      <td>8.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>9.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>9.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>9.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>9.901900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>8.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1383</td>\n",
       "      <td>8.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1384</td>\n",
       "      <td>7.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385</td>\n",
       "      <td>7.636600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>8.185900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1387</td>\n",
       "      <td>8.829900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1388</td>\n",
       "      <td>8.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1389</td>\n",
       "      <td>7.916700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>9.012600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1391</td>\n",
       "      <td>8.931100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1392</td>\n",
       "      <td>8.378500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1393</td>\n",
       "      <td>8.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1394</td>\n",
       "      <td>8.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395</td>\n",
       "      <td>8.299300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1396</td>\n",
       "      <td>8.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1397</td>\n",
       "      <td>8.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1398</td>\n",
       "      <td>8.442500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1399</td>\n",
       "      <td>9.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>8.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1401</td>\n",
       "      <td>9.137400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1402</td>\n",
       "      <td>9.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1403</td>\n",
       "      <td>8.717100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1404</td>\n",
       "      <td>8.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405</td>\n",
       "      <td>7.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1406</td>\n",
       "      <td>8.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1407</td>\n",
       "      <td>8.830100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1408</td>\n",
       "      <td>9.230000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1409</td>\n",
       "      <td>9.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>9.662500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1411</td>\n",
       "      <td>9.117100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1412</td>\n",
       "      <td>8.697400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1413</td>\n",
       "      <td>7.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1414</td>\n",
       "      <td>8.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415</td>\n",
       "      <td>8.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1416</td>\n",
       "      <td>8.776500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1417</td>\n",
       "      <td>8.133300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1418</td>\n",
       "      <td>7.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1419</td>\n",
       "      <td>8.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>8.742400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1421</td>\n",
       "      <td>7.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1422</td>\n",
       "      <td>7.838300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1423</td>\n",
       "      <td>8.436000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1424</td>\n",
       "      <td>8.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>9.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1426</td>\n",
       "      <td>8.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1427</td>\n",
       "      <td>8.934000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1428</td>\n",
       "      <td>8.876700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1429</td>\n",
       "      <td>8.540400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>8.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1431</td>\n",
       "      <td>8.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1432</td>\n",
       "      <td>8.321300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1433</td>\n",
       "      <td>7.879100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1434</td>\n",
       "      <td>8.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435</td>\n",
       "      <td>8.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1436</td>\n",
       "      <td>7.703800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1437</td>\n",
       "      <td>7.837800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1438</td>\n",
       "      <td>8.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1439</td>\n",
       "      <td>9.070500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>8.741500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1441</td>\n",
       "      <td>8.821500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1442</td>\n",
       "      <td>9.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1443</td>\n",
       "      <td>9.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1444</td>\n",
       "      <td>8.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445</td>\n",
       "      <td>7.946800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1446</td>\n",
       "      <td>8.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1447</td>\n",
       "      <td>9.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1448</td>\n",
       "      <td>8.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1449</td>\n",
       "      <td>8.178800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>9.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1451</td>\n",
       "      <td>9.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1452</td>\n",
       "      <td>7.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1453</td>\n",
       "      <td>9.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1454</td>\n",
       "      <td>9.070200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455</td>\n",
       "      <td>9.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1456</td>\n",
       "      <td>8.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1457</td>\n",
       "      <td>7.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1458</td>\n",
       "      <td>8.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1459</td>\n",
       "      <td>9.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>8.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1461</td>\n",
       "      <td>9.132900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1462</td>\n",
       "      <td>9.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1463</td>\n",
       "      <td>7.859300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1464</td>\n",
       "      <td>8.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465</td>\n",
       "      <td>8.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1466</td>\n",
       "      <td>8.876800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1467</td>\n",
       "      <td>9.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1468</td>\n",
       "      <td>8.414800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1469</td>\n",
       "      <td>8.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>8.370100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1471</td>\n",
       "      <td>7.528500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1472</td>\n",
       "      <td>8.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1473</td>\n",
       "      <td>9.654500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1474</td>\n",
       "      <td>10.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>9.399500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1476</td>\n",
       "      <td>8.659200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1477</td>\n",
       "      <td>9.597700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1478</td>\n",
       "      <td>8.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1479</td>\n",
       "      <td>9.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>9.691800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1481</td>\n",
       "      <td>8.894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1482</td>\n",
       "      <td>8.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1483</td>\n",
       "      <td>10.309400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1484</td>\n",
       "      <td>9.196900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485</td>\n",
       "      <td>8.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1486</td>\n",
       "      <td>9.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1487</td>\n",
       "      <td>8.829800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1488</td>\n",
       "      <td>8.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1489</td>\n",
       "      <td>8.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>10.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1491</td>\n",
       "      <td>7.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1492</td>\n",
       "      <td>9.477900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1493</td>\n",
       "      <td>8.578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1494</td>\n",
       "      <td>9.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495</td>\n",
       "      <td>8.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1496</td>\n",
       "      <td>8.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1497</td>\n",
       "      <td>9.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1498</td>\n",
       "      <td>8.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1499</td>\n",
       "      <td>8.492400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>8.663700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1501</td>\n",
       "      <td>9.521600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1502</td>\n",
       "      <td>8.839300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1503</td>\n",
       "      <td>8.913800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1504</td>\n",
       "      <td>8.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505</td>\n",
       "      <td>8.314500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1506</td>\n",
       "      <td>9.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1507</td>\n",
       "      <td>8.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1508</td>\n",
       "      <td>8.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509</td>\n",
       "      <td>9.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>10.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1511</td>\n",
       "      <td>9.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1512</td>\n",
       "      <td>9.618600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1513</td>\n",
       "      <td>9.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1514</td>\n",
       "      <td>9.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515</td>\n",
       "      <td>8.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1516</td>\n",
       "      <td>9.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1517</td>\n",
       "      <td>8.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1518</td>\n",
       "      <td>9.231700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1519</td>\n",
       "      <td>9.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>8.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521</td>\n",
       "      <td>8.977700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1522</td>\n",
       "      <td>8.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1523</td>\n",
       "      <td>10.106900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1524</td>\n",
       "      <td>7.508200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>8.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1526</td>\n",
       "      <td>7.976600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1527</td>\n",
       "      <td>8.773100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1528</td>\n",
       "      <td>9.087600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1529</td>\n",
       "      <td>8.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>8.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1531</td>\n",
       "      <td>9.432700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1532</td>\n",
       "      <td>9.173200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1533</td>\n",
       "      <td>7.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1534</td>\n",
       "      <td>9.094900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1535</td>\n",
       "      <td>9.059900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1536</td>\n",
       "      <td>9.673900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1537</td>\n",
       "      <td>7.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1538</td>\n",
       "      <td>8.883200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1539</td>\n",
       "      <td>9.309200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>9.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1541</td>\n",
       "      <td>7.767300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1542</td>\n",
       "      <td>8.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1543</td>\n",
       "      <td>8.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1544</td>\n",
       "      <td>8.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1545</td>\n",
       "      <td>8.753000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1546</td>\n",
       "      <td>9.018300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1547</td>\n",
       "      <td>10.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1548</td>\n",
       "      <td>8.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1549</td>\n",
       "      <td>8.393900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>9.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1551</td>\n",
       "      <td>9.495300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1552</td>\n",
       "      <td>8.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1553</td>\n",
       "      <td>8.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1554</td>\n",
       "      <td>8.812900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1555</td>\n",
       "      <td>10.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1556</td>\n",
       "      <td>8.541800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1557</td>\n",
       "      <td>8.560100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1558</td>\n",
       "      <td>9.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1559</td>\n",
       "      <td>9.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>9.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1561</td>\n",
       "      <td>8.885300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1562</td>\n",
       "      <td>8.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1563</td>\n",
       "      <td>8.698300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1564</td>\n",
       "      <td>8.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1565</td>\n",
       "      <td>7.836200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1566</td>\n",
       "      <td>9.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1567</td>\n",
       "      <td>7.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1568</td>\n",
       "      <td>10.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>7.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>9.879300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1571</td>\n",
       "      <td>9.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1572</td>\n",
       "      <td>9.315100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1573</td>\n",
       "      <td>8.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1574</td>\n",
       "      <td>8.434800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>9.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1576</td>\n",
       "      <td>9.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1577</td>\n",
       "      <td>9.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578</td>\n",
       "      <td>8.824800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1579</td>\n",
       "      <td>8.296000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>9.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1581</td>\n",
       "      <td>8.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1582</td>\n",
       "      <td>7.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1583</td>\n",
       "      <td>9.794500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1584</td>\n",
       "      <td>8.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1585</td>\n",
       "      <td>10.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1586</td>\n",
       "      <td>8.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1587</td>\n",
       "      <td>9.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1588</td>\n",
       "      <td>9.037600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1589</td>\n",
       "      <td>8.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>8.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1591</td>\n",
       "      <td>8.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1592</td>\n",
       "      <td>10.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1593</td>\n",
       "      <td>8.210800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1594</td>\n",
       "      <td>10.104600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1595</td>\n",
       "      <td>9.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1596</td>\n",
       "      <td>10.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1597</td>\n",
       "      <td>8.934600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1598</td>\n",
       "      <td>8.453400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1599</td>\n",
       "      <td>10.819300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>8.309500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1601</td>\n",
       "      <td>8.695700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1602</td>\n",
       "      <td>9.491300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1603</td>\n",
       "      <td>8.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1604</td>\n",
       "      <td>9.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1605</td>\n",
       "      <td>7.939300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1606</td>\n",
       "      <td>7.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1607</td>\n",
       "      <td>8.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1608</td>\n",
       "      <td>7.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1609</td>\n",
       "      <td>11.998100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>8.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611</td>\n",
       "      <td>9.047900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1612</td>\n",
       "      <td>9.082900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1613</td>\n",
       "      <td>9.354400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1614</td>\n",
       "      <td>8.379000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1615</td>\n",
       "      <td>8.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1616</td>\n",
       "      <td>10.166200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1617</td>\n",
       "      <td>7.882000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1618</td>\n",
       "      <td>10.169000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1619</td>\n",
       "      <td>8.801700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>8.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1621</td>\n",
       "      <td>8.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1622</td>\n",
       "      <td>9.618900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1623</td>\n",
       "      <td>8.593900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1624</td>\n",
       "      <td>8.137100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>10.367500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1626</td>\n",
       "      <td>8.779400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1627</td>\n",
       "      <td>9.631500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1628</td>\n",
       "      <td>8.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1629</td>\n",
       "      <td>7.897600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>8.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1631</td>\n",
       "      <td>8.814700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1632</td>\n",
       "      <td>9.378800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1633</td>\n",
       "      <td>8.870100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1634</td>\n",
       "      <td>9.560600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1635</td>\n",
       "      <td>9.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1636</td>\n",
       "      <td>10.311900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1637</td>\n",
       "      <td>8.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1638</td>\n",
       "      <td>8.600600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1639</td>\n",
       "      <td>8.203000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>7.569500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1641</td>\n",
       "      <td>8.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1642</td>\n",
       "      <td>8.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1643</td>\n",
       "      <td>8.048900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1644</td>\n",
       "      <td>10.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1645</td>\n",
       "      <td>9.354200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1646</td>\n",
       "      <td>8.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1647</td>\n",
       "      <td>8.609200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1648</td>\n",
       "      <td>7.765500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1649</td>\n",
       "      <td>9.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>10.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1651</td>\n",
       "      <td>8.537900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1652</td>\n",
       "      <td>9.018000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1653</td>\n",
       "      <td>8.289500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1654</td>\n",
       "      <td>7.942000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1655</td>\n",
       "      <td>8.331200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1656</td>\n",
       "      <td>9.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1657</td>\n",
       "      <td>8.282000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>7.840700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1659</td>\n",
       "      <td>8.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>9.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661</td>\n",
       "      <td>9.714100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1662</td>\n",
       "      <td>8.913600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1663</td>\n",
       "      <td>7.576700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1664</td>\n",
       "      <td>7.687500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1665</td>\n",
       "      <td>9.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1666</td>\n",
       "      <td>8.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1667</td>\n",
       "      <td>8.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1668</td>\n",
       "      <td>9.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1669</td>\n",
       "      <td>7.384800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>8.916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1671</td>\n",
       "      <td>9.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1672</td>\n",
       "      <td>8.168700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1673</td>\n",
       "      <td>8.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1674</td>\n",
       "      <td>9.332700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>8.596500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1676</td>\n",
       "      <td>8.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1677</td>\n",
       "      <td>8.831400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1678</td>\n",
       "      <td>8.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1679</td>\n",
       "      <td>8.982100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>8.711300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1681</td>\n",
       "      <td>7.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1682</td>\n",
       "      <td>8.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1683</td>\n",
       "      <td>8.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1684</td>\n",
       "      <td>8.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1685</td>\n",
       "      <td>8.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1686</td>\n",
       "      <td>7.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1687</td>\n",
       "      <td>9.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1688</td>\n",
       "      <td>8.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1689</td>\n",
       "      <td>9.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>9.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1691</td>\n",
       "      <td>8.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1692</td>\n",
       "      <td>9.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1693</td>\n",
       "      <td>9.519900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1694</td>\n",
       "      <td>8.393800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1695</td>\n",
       "      <td>8.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1696</td>\n",
       "      <td>9.408600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1697</td>\n",
       "      <td>9.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1698</td>\n",
       "      <td>7.747700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1699</td>\n",
       "      <td>9.671200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>7.974900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1701</td>\n",
       "      <td>8.340500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1702</td>\n",
       "      <td>9.902800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1703</td>\n",
       "      <td>9.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1704</td>\n",
       "      <td>8.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1705</td>\n",
       "      <td>9.080400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1706</td>\n",
       "      <td>9.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1707</td>\n",
       "      <td>9.387000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1708</td>\n",
       "      <td>8.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1709</td>\n",
       "      <td>8.814900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>10.214200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1711</td>\n",
       "      <td>8.010400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1712</td>\n",
       "      <td>9.633300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1713</td>\n",
       "      <td>7.993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1714</td>\n",
       "      <td>6.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1715</td>\n",
       "      <td>9.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1716</td>\n",
       "      <td>9.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1717</td>\n",
       "      <td>9.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1718</td>\n",
       "      <td>8.271200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1719</td>\n",
       "      <td>9.857800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>8.745100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1721</td>\n",
       "      <td>8.908500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1722</td>\n",
       "      <td>8.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1723</td>\n",
       "      <td>8.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1724</td>\n",
       "      <td>8.168600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>8.386800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1726</td>\n",
       "      <td>8.108100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1727</td>\n",
       "      <td>9.556200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1728</td>\n",
       "      <td>8.826600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1729</td>\n",
       "      <td>8.141600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>9.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1731</td>\n",
       "      <td>8.321600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1732</td>\n",
       "      <td>8.137300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1733</td>\n",
       "      <td>8.380100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1734</td>\n",
       "      <td>8.555800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1735</td>\n",
       "      <td>8.813400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1736</td>\n",
       "      <td>8.337200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1737</td>\n",
       "      <td>10.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1738</td>\n",
       "      <td>9.507300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1739</td>\n",
       "      <td>9.807400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>8.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1741</td>\n",
       "      <td>8.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1742</td>\n",
       "      <td>9.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1743</td>\n",
       "      <td>8.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744</td>\n",
       "      <td>9.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745</td>\n",
       "      <td>8.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1746</td>\n",
       "      <td>8.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1747</td>\n",
       "      <td>8.998300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1748</td>\n",
       "      <td>9.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749</td>\n",
       "      <td>9.141300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>7.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1751</td>\n",
       "      <td>9.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1752</td>\n",
       "      <td>7.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1753</td>\n",
       "      <td>9.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1754</td>\n",
       "      <td>9.650200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1755</td>\n",
       "      <td>8.473000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756</td>\n",
       "      <td>8.616300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1757</td>\n",
       "      <td>9.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1758</td>\n",
       "      <td>9.173900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1759</td>\n",
       "      <td>8.619300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>8.749800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1761</td>\n",
       "      <td>7.797100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1762</td>\n",
       "      <td>8.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1763</td>\n",
       "      <td>8.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1764</td>\n",
       "      <td>8.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1765</td>\n",
       "      <td>9.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1766</td>\n",
       "      <td>9.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1767</td>\n",
       "      <td>8.342000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1768</td>\n",
       "      <td>8.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1769</td>\n",
       "      <td>9.044100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>8.106800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1771</td>\n",
       "      <td>8.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1772</td>\n",
       "      <td>9.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1773</td>\n",
       "      <td>8.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1774</td>\n",
       "      <td>8.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>9.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1776</td>\n",
       "      <td>9.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1777</td>\n",
       "      <td>9.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1778</td>\n",
       "      <td>8.778100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1779</td>\n",
       "      <td>9.581800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>8.375900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1781</td>\n",
       "      <td>8.573400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1782</td>\n",
       "      <td>8.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1783</td>\n",
       "      <td>9.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1784</td>\n",
       "      <td>8.791300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1785</td>\n",
       "      <td>9.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1786</td>\n",
       "      <td>7.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1787</td>\n",
       "      <td>8.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1788</td>\n",
       "      <td>8.266300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1789</td>\n",
       "      <td>8.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>8.431400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1791</td>\n",
       "      <td>9.489900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1792</td>\n",
       "      <td>8.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1793</td>\n",
       "      <td>9.368200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1794</td>\n",
       "      <td>9.377400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1795</td>\n",
       "      <td>8.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1796</td>\n",
       "      <td>8.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1797</td>\n",
       "      <td>8.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1798</td>\n",
       "      <td>8.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1799</td>\n",
       "      <td>8.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>8.713800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1801</td>\n",
       "      <td>9.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1802</td>\n",
       "      <td>9.477700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1803</td>\n",
       "      <td>8.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1804</td>\n",
       "      <td>7.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1805</td>\n",
       "      <td>8.911800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1806</td>\n",
       "      <td>8.779900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1807</td>\n",
       "      <td>9.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1808</td>\n",
       "      <td>7.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1809</td>\n",
       "      <td>7.684700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>10.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1811</td>\n",
       "      <td>8.823300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1812</td>\n",
       "      <td>8.409600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1813</td>\n",
       "      <td>7.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1814</td>\n",
       "      <td>9.106200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1815</td>\n",
       "      <td>7.897400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>8.053500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>8.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>9.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>8.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>8.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1821</td>\n",
       "      <td>10.192400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1822</td>\n",
       "      <td>8.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1823</td>\n",
       "      <td>8.475100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1824</td>\n",
       "      <td>9.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>9.041200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1826</td>\n",
       "      <td>9.170100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1827</td>\n",
       "      <td>8.554900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1828</td>\n",
       "      <td>8.938800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1829</td>\n",
       "      <td>8.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>7.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1831</td>\n",
       "      <td>8.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1832</td>\n",
       "      <td>8.803600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1833</td>\n",
       "      <td>8.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1834</td>\n",
       "      <td>7.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1835</td>\n",
       "      <td>8.569100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7755, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1381, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5180, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8471, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.6778, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.6719, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7453, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4755, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0492, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7755, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8482, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4335, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7446, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3458, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7341, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0291, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7573, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5990, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4151, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9334, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7511, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0885, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7449, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8098, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9142, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7159, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6672, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4898, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0986, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4207, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4869, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0776, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4146, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4248, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7167, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8993, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6989, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2990, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7723, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1098, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2266, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9359, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3419, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4862, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7262, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3731, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8204, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8118, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7607, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4760, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1467, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6965, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6388, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4920, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0770, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7979, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1814, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4780, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0773, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5417, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1381, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2939, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5568, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0916, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8557, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4454, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0987, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3395, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2954, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5557, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2081, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7606, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0584, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7332, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0963, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4216, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7364, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1720, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7445, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0824, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4339, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7179, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3692, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3740, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7425, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3120, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7478, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7771, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2559, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1838, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3232, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7265, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7338, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7099, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6917, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4613, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7778, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4077, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2527, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7398, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5675, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1940, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1986, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7804, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7205, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3228, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7290, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6276, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3997, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7386, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3510, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6884, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1177, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5611, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7314, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3030, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7284, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5310, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5148, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0921, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8301, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7953, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6737, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1653, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5795, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8244, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4113, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7272, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8421, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7356, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4101, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0464, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8074, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7266, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4398, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4777, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7778, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4724, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2179, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7036, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3754, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4605, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7451, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0174, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6301, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7157, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2856, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1473, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4194, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7622, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6732, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7655, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3010, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3185, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7630, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1774, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2416, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7428, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7255, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1735, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7609, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7691, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2290, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4948, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7180, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6030, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7785, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5493, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5308, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4050, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7165, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3080, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0820, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7411, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5215, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3915, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7269, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4874, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0176, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5104, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8319, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7624, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0918, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5975, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3741, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7982, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3290, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3337, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7915, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2549, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4708, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7845, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2048, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3371, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7544, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5159, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2940, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3189, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3098, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0772, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9488, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4433, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7736, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0767, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4971, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3471, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7661, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1095, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7752, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1096, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3556, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3784, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0623, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1163, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3333, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6496, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7777, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7252, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4587, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0902, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4643, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3712, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0110, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5179, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7751, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7526, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1091, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3487, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4373, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4433, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8141, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2961, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7293, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3631, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7326, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1787, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3836, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7235, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0986, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3355, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3038, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7266, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0782, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6797, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0127, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3138, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2637, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7176, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9980, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7536, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7637, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2515, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7466, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3763, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3997, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5422, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7723, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3678, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7487, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2152, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7517, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5001, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1743, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7475, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2693, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5214, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7636, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7642, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.2541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7543, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2910, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5220, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7421, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0747, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4354, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7341, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9061, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7019, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7937, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7701, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0770, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3994, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3398, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1070, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7234, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3405, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7516, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0924, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1117, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7036, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7316, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4098, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7454, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0777, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0353, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8870, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7738, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1731, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6724, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0088, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7334, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4311, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1963, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7856, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1796, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4551, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3303, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7963, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3578, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7478, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3584, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3135, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7708, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0923, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3572, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6966, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7593, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0894, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3639, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6943, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7834, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5537, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7566, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0777, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7127, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3533, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7028, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4384, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7501, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7276, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7503, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1511, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2464, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2164, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7350, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2689, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7523, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4707, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2913, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7484, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7949, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7274, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7109, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7246, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9523, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7490, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4925, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7499, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1568, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4354, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8561, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0804, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1985, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6869, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7406, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7217, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4775, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4168, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7380, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7713, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5431, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5209, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5087, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7257, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1041, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7130, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1430, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6712, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5335, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6484, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7035, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3503, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9193, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7222, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6095, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0369, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5971, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7235, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2144, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4305, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4484, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7113, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0211, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1517, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0539, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7552, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8208, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5160, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1425, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2459, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7934, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5258, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0346, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7655, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0800, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2768, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7243, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0883, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7507, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8866, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7065, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8228, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0290, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7867, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1509, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7305, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1294, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4993, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3015, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7536, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1657, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5273, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7578, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4593, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5689, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5658, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7439, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5091, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5194, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7516, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1464, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7349, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1318, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0525, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2320, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7518, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3060, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3440, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7481, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9785, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0703, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7426, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3461, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5698, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7147, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4858, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7136, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9248, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4580, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7559, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4332, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7021, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5643, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2579, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6894, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4853, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7395, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6826, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5355, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3290, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2536, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1515, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7744, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0931, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4823, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0820, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2289, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7255, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0822, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5820, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7529, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7108, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7969, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7969, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0898, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2788, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3972, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7549, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0822, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2821, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2478, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7358, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0947, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2385, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7682, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2659, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7190, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3957, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3064, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7150, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6059, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7278, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0832, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6081, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7398, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3367, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2482, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7120, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3505, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7716, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6108, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7740, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5013, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1806, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7316, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3154, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3612, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7238, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1029, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4093, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4995, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9194, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3489, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6966, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7604, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.4079, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3006, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-14., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7379, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6177, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9907, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2416, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3567, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-15.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7631, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1973, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7749, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4083, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4698, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2555, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7576, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6898, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7045, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6976, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7081, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8001, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7639, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0975, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.4520, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0414, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-14., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7185, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6414, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0852, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2533, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7593, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7127, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4928, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5431, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7513, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1448, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7181, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1246, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5866, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6945, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3854, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2221, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4819, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7761, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1639, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7098, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2359, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7997, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5049, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2056, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0868, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2468, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2133, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3071, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2662, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7255, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4520, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6373, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7644, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7021, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.3224, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7603, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0864, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6192, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5708, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7248, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1084, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3048, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7568, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4042, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0868, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3166, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7368, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8148, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1885, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1322, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1919, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7101, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0653, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1401, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1551, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7180, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3483, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7794, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4521, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7222, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0747, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9264, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5793, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0744, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.2229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7386, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1305, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3213, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7966, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9490, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0363, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7366, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5135, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4369, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2410, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7234, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4648, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5171, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7662, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4499, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7497, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7968, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5443, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7248, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0745, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1693, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4667, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7437, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7592, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9474, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2511, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7266, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0744, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8437, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7253, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5182, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3054, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7541, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0878, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5825, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1108, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2493, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9573, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1910, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0920, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6198, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4213, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7701, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5636, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2436, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7382, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3817, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7361, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6168, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7475, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4336, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2349, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7453, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2231, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7340, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5377, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2241, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7585, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2284, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7278, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2504, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0909, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6550, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1493, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7507, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3899, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2341, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7457, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0767, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5163, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1612, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7336, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6082, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7331, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0899, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7364, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6618, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7445, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0767, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4819, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2733, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0991, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2868, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7405, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6019, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7865, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0309, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8318, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1306, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2831, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5629, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6133, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0913, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9328, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5593, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7193, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1183, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1287, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1736, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2796, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5896, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7115, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6588, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7469, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8838, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7228, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6464, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5283, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5214, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0868, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2958, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7029, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2291, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7395, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1114, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1304, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7728, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0777, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7386, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0935, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6238, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0940, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8711, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7495, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1637, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2979, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7683, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2964, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0944, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3778, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3566, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7427, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0996, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9377, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6198, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7957, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0623, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8646, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0947, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7719, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3681, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7436, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9470, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5721, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8110, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0767, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7765, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6871, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7581, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4805, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7853, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7854, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8877, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7495, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1004, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7874, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7310, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0767, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8286, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4395, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7451, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6637, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9597, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7269, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0975, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3891, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5002, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7602, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9146, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7802, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9271, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7273, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7939, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8042, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0967, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2940, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1931, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7780, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7559, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0773, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9842, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7930, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0978, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4802, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8136, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7701, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1059, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1907, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7722, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4220, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3739, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0998, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4136, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1001, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7779, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8539, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3355, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8024, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0782, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5283, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7579, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0780, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8225, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1018, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8057, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1932, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7349, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3811, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7620, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3286, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1277, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7734, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1771, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2559, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6507, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5332, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7467, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1009, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2101, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7745, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0833, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4457, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8054, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7733, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7656, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7497, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2037, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0782, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7138, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8347, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5355, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9210, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7956, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9433, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6902, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7941, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5432, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2278, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7307, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1029, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4547, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1830, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7378, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3382, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3118, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7089, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2294, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8076, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2395, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2300, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0785, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5730, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7021, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7993, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6917, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5368, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0789, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3095, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7346, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1191, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4228, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7980, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1043, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0683, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7742, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7427, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0799, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7506, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9510, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2710, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7508, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3984, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7540, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8179, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1027, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5083, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8035, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0800, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7616, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2563, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7854, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9136, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7882, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1073, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2569, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3295, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0790, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1995, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2985, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.6839, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0793, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4666, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1190, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7668, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0789, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3618, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9676, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7876, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0790, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6414, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3518, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0284, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4929, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8010, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2169, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9213, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8358, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4234, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0196, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8583, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0796, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6746, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3504, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8052, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1094, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1589, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1271, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3154, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7788, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1028, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9865, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3646, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7779, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1100, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6023, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3234, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8343, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1101, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6853, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0902, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0793, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5251, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5996, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7577, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0801, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1350, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1115, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3752, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5871, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1508, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3331, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1428, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8051, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1111, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7988, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0801, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0075, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7136, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0804, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2356, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8739, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7672, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0810, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0324, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4311, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7530, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3974, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8084, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1120, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5593, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0195, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1033, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0441, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0798, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4956, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4893, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0811, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0682, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7602, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0818, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1348, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3797, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7828, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0820, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0795, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0479, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0951, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6028, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3878, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7616, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0797, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8973, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3922, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1117, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1635, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7786, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0822, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3011, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.8906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7759, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2009, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3384, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0100, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5167, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8044, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8052, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6189, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2770, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7969, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8903, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5468, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8009, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1154, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1691, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4941, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7865, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1155, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0643, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1560, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0861, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0679, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0715, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7714, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1166, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6223, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0823, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8054, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7884, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0833, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2724, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7873, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5787, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7233, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0835, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2220, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2404, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8006, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6301, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4379, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8174, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0835, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0513, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2791, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7676, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0834, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7342, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4835, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7758, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1953, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7961, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2645, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2632, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8020, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0853, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5068, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9511, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8350, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5800, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5168, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7541, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0846, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1577, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7730, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1464, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7061, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6478, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4627, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1425, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7862, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4548, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7708, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1694, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4174, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0967, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8077, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1217, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3919, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4098, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7988, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0855, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5636, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3722, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8246, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0854, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4623, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7948, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0856, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4059, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1961, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7909, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9169, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4119, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0865, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2403, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0862, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3314, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7821, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0861, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3186, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1246, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4367, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7112, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8199, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1935, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9814, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7118, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0864, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6471, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0866, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2073, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8214, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1261, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6522, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7856, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6603, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4696, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9683, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5201, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1283, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3074, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1280, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0871, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7223, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9501, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7903, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1020, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6324, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5012, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7965, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0871, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3440, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8275, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5293, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3131, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8304, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8717, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0870, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3666, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3384, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8249, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8441, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3618, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5525, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8342, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2457, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7653, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1310, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1708, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8255, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0897, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2140, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7322, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5138, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8080, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1321, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.4185, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7707, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1192, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3248, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8021, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0889, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3523, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3315, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0890, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5707, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8341, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1080, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.7126, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1019, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5144, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1400, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1728, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0883, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8417, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0893, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4910, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2841, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8186, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3898, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8374, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7052, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0877, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3546, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3740, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1379, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2077, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7695, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3467, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7911, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7969, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7786, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1371, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2876, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8463, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0887, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8646, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7775, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0896, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1653, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9933, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0887, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0407, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7805, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0895, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8472, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1074, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4341, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2073, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7964, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0888, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6074, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7714, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1394, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4748, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8317, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1395, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9358, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8071, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0897, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8289, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8080, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1400, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3168, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7399, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7344, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0891, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1711, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0600, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0894, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2790, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8477, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0900, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8603, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1414, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0588, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3530, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8176, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8030, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2286, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8463, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1426, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3361, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6321, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2865, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8916, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3284, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7882, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2988, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3574, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8099, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0915, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6180, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8462, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1439, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3781, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7349, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1451, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4895, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1183, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6838, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8645, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1450, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8395, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0907, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6579, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8191, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0910, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9179, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1477, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4074, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3272, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8276, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0918, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7687, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7656, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(8.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7782, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4696, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5537, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1487, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2584, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2373, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7811, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0918, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7796, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0912, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7790, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2294, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8110, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0924, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4828, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1501, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4330, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2184, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8263, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3982, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7539, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0922, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8520, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1516, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2217, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2143, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8014, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6452, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5210, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8326, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1253, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2698, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7886, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0932, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7487, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8298, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0923, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0992, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8912, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1991, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3452, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1537, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6543, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1376, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1848, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7829, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5161, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7716, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1552, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4719, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7672, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7859, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2546, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8030, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0944, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6292, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2806, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8418, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1411, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8197, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1678, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0791, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8149, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5567, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4666, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1186, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5603, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4669, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8236, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0948, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3606, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0947, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4204, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5039, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8396, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6767, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8324, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1586, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6657, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8263, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0447, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3965, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1586, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3561, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8236, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1593, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8192, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0985, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3922, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1971, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2122, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8291, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8479, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0990, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8233, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4181, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2877, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8237, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0659, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7994, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1486, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7991, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2173, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4940, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8900, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0735, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2985, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1082, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1383, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7894, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0957, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3154, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2990, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7999, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1216, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1911, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8456, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1613, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8789, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8057, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1824, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1336, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8097, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0969, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1306, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1019, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3713, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7596, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7430, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0785, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7725, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7666, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0972, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0577, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7865, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0965, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3566, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1217, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3426, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1108, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7887, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0971, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8642, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0539, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7991, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1633, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5252, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8238, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1164, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4049, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4588, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7703, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1794, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1766, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8511, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1736, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8332, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0975, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0313, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4269, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8274, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1017, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.2215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8925, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1647, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5873, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0858, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9544, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2265, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8640, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2248, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7544, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2671, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.7031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8810, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8720, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3517, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8484, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7836, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5401, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8515, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1405, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3369, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3124, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8602, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8419, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1666, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7894, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8252, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1664, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5852, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8732, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1196, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3958, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8883, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1093, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0998, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8113, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8517, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2989, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8404, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1689, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3459, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9719, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1682, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8729, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3245, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3013, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1437, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1214, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2089, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1695, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9143, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1323, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3084, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2264, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8349, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7912, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1004, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1112, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2421, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3902, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8614, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.0999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5942, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8441, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1743, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3525, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4857, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7858, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1726, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5624, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8909, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1706, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0169, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3667, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8020, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1287, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3415, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3725, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8575, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1019, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2035, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9079, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2111, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8274, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1091, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1909, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8932, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1124, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0004, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8658, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1724, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1734, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8270, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1011, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2653, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4935, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8654, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8645, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6804, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8980, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3941, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7785, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1018, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7327, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1180, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7757, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1522, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5927, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2174, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5467, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1683, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2710, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8175, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8559, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8459, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1020, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5236, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1538, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8842, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1514, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2943, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1211, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8165, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1406, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4863, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2381, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8574, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7102, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3106, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8592, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1458, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1110, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1033, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9269, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8556, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1396, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3316, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7457, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1031, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2927, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8151, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1821, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3211, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8902, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1023, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8109, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8364, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1030, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3448, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1027, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4270, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2251, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5258, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0367, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1754, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6984, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9430, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3509, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1790, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8019, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1954, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6424, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8156, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8017, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1776, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6244, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2067, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1780, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3406, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8819, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1235, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8621, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1366, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5207, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3112, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8802, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2030, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2743, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8093, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1357, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4073, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0482, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8339, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1158, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7847, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4546, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1452, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8129, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8970, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1400, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2389, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4525, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8283, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3374, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8022, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1044, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6332, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9527, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4957, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8638, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8526, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1034, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2413, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3370, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7813, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5960, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8432, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2719, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8874, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1045, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1793, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2864, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5025, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8451, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1027, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8602, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1937, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3557, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1073, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1645, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6174, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4523, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2264, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1208, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4832, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9637, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1822, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4468, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8474, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1799, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3980, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3264, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7990, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.3044, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2890, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8593, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7817, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4105, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1821, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3690, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8896, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1057, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2596, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8116, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1173, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0826, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8415, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1831, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6712, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7640, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1056, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2118, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7958, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1066, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7908, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8368, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1564, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2944, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7444, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1058, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8348, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1553, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9675, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3537, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1517, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7427, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9131, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1929, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4920, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8863, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1168, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8357, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1684, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4191, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0467, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8495, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3103, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8293, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1591, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2209, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8801, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8933, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5407, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1854, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2869, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0839, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8010, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1768, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4457, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1371, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4997, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1356, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4611, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8418, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1162, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6199, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1126, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5789, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8337, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1861, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4381, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2661, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7667, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2441, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5271, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1083, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3659, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0984, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1467, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2129, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1429, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1972, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5198, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1559, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8738, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1087, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2096, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7823, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1398, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6805, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8790, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1766, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0050, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8499, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1081, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0874, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1884, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1991, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1058, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9120, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4829, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8261, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1085, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7631, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5733, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9446, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1951, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5090, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8346, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1331, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4435, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8858, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4165, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6242, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8796, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2100, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2940, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3137, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8383, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1103, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7784, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4238, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2032, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9691, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7996, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3030, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7297, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1865, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1093, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5186, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8732, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5890, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1670, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8567, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1090, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2213, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1745, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3161, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3294, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7934, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1776, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4803, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1085, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9446, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8808, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1484, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2624, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8477, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1586, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3237, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1832, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1709, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4244, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3841, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8722, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4283, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1404, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8445, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1090, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5474, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3627, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5407, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7005, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1650, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1836, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8223, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8704, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2033, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6711, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8973, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1456, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9876, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0775, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8499, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1193, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7800, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0560, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1899, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2849, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4406, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8045, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2955, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1952, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9611, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8279, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3424, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0264, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3184, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2060, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6299, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1526, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8433, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1102, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9973, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1102, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5813, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2307, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2063, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3741, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8870, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1101, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1606, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3547, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1236, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0900, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7148, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8648, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1099, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5719, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8589, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1942, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0606, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8629, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8665, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1116, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8460, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1941, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9405, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1106, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2201, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8249, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3140, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9835, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8871, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4033, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9901, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8794, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1539, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5649, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8709, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8325, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1530, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6015, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1941, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8793, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5549, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0264, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0729, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7703, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1113, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1655, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4832, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9280, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1538, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2775, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9663, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8499, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1410, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8740, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4720, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9739, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8280, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1671, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4480, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8661, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1773, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4686, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4490, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9028, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1955, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7948, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1855, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5291, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8414, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1774, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7627, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8891, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0935, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9470, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2102, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2480, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2527, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5134, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1432, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1999, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1663, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8134, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1960, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7451, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1110, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4734, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2656, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1374, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8432, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8603, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8718, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1982, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2348, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7933, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5836, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3078, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1735, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1838, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0163, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8974, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2311, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2910, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9049, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2025, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8449, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1407, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1115, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2075, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5886, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1376, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5414, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1121, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5954, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8772, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2290, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8243, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3968, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8859, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1601, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2519, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8390, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1121, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9172, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2239, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8662, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2539, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9163, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9484, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4490, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6470, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8968, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2463, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8383, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1732, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8978, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3849, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1944, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9260, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2108, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1173, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5241, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1997, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2795, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9405, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1648, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3150, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0730, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8118, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5070, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9131, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1726, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4181, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2129, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1964, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2540, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8245, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1122, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4397, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9957, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1708, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0408, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9620, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8327, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6193, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9204, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7930, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3843, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1686, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1628, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8523, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2480, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8748, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8076, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1132, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4480, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6137, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0907, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8839, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2066, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4972, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4106, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9221, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2011, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0979, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8895, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6224, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8704, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3558, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6423, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9384, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4644, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3276, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8838, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1660, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6780, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6909, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8760, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1582, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3218, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4526, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9093, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2248, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9415, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2495, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9119, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0607, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1526, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0360, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8251, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1459, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1187, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3344, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9995, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2403, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1144, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8085, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1777, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8362, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1363, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3520, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4458, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8043, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2035, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7147, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9629, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1911, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3523, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9142, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2117, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0088, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8899, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2109, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0036, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2217, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5930, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7865, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1143, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9449, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8835, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2632, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1359, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8105, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2043, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5539, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8335, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1669, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8347, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2605, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6091, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8369, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1386, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3494, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1164, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6115, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4018, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8714, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1202, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4673, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3094, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9821, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2407, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1415, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2060, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9824, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9019, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2423, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4189, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1015, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8460, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1836, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2876, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9984, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2322, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4471, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1639, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8862, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2332, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9864, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8994, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2058, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1110, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8781, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2720, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5065, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8780, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1149, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8361, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1155, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4588, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2903, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2063, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8274, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2065, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4033, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7867, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5863, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2068, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.4003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1413, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2453, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1568, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4141, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8277, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2855, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1658, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8474, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1210, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5736, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1768, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9006, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1979, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2863, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9540, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2070, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5150, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8579, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2193, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9822, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2586, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1170, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1068, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1157, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(13.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8810, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1681, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2977, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2138, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6808, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8960, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1760, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0819, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2094, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2969, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8275, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5743, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4209, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7996, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1158, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2687, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2931, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1168, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2327, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9388, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2558, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9257, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2067, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1578, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7810, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1917, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5087, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8836, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2089, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6330, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8334, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4016, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1086, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9141, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2198, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5299, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2818, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8530, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8484, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1325, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8689, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1496, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9317, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7696, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1513, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6216, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8871, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5135, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9218, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1454, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1336, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8732, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5155, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1913, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2095, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2702, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4291, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1575, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8282, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1794, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8354, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2197, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3729, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0064, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8760, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2107, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9286, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2203, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4639, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1191, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8754, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9946, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0926, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2095, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3157, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2590, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8275, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1624, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4155, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5041, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8993, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2246, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3662, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2690, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8261, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1576, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5143, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8590, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1200, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9432, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2042, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8669, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9713, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6360, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2101, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0128, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8915, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2044, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4421, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2512, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9820, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3050, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9403, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2006, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3307, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8115, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2115, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3941, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8120, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1176, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2323, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1318, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9548, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2109, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0654, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1565, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8776, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2118, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2611, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0720, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9115, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1323, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0341, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8174, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1169, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1554, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5032, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9518, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3242, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0141, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9096, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2327, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2123, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2162, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4432, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8533, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1346, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9047, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2577, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5124, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3141, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9522, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2056, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3693, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4133, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9128, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9184, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8647, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8986, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3338, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0739, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8016, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2127, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3661, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3039, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9378, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2325, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4212, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0581, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8215, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1819, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3740, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2846, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8352, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8281, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8970, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2134, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3036, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1060, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8039, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2504, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2060, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9076, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1834, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3508, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8366, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5557, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8223, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1170, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8582, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2421, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9205, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3597, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2155, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3704, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8415, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8366, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1532, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1336, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5518, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1609, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1049, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2773, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8573, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4546, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8247, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2161, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2745, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4759, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3848, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8857, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2142, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3583, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9272, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2868, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2940, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8733, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1637, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0749, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5632, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2277, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3638, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8483, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1998, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7734, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8756, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1921, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4163, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8097, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8039, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8282, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2012, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8758, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5635, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8533, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1998, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3241, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1980, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9814, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1483, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9828, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2264, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2174, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0441, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2585, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2882, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1555, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8886, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2345, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0898, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5601, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9179, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5253, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4762, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8768, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7944, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1828, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2623, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8596, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1372, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0803, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1727, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3070, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2677, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8074, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1203, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3596, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1724, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9552, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2331, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8426, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2150, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3225, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1965, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7525, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3350, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8704, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2271, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2436, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8795, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1331, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1478, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8198, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3547, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9413, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1502, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7026, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1926, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2037, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3355, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8419, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2492, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9017, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2251, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8745, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6328, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2162, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2990, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9658, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1976, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9961, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9221, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2275, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2879, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2383, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8540, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2371, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1884, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8619, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2190, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7793, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3827, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8411, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1183, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9506, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8730, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2055, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7687, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3044, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8275, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2377, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8408, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8549, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8018, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2056, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8168, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9739, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7477, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7499, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1509, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6047, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8087, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2716, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4399, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9117, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4752, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8483, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8646, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2409, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4515, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2591, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8853, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7858, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2177, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0884, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1724, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0826, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2993, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6952, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2148, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8582, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1982, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2584, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1862, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1489, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8544, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3175, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9056, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1608, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3232, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8707, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1693, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0888, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1564, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9260, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5785, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8660, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2165, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4889, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0151, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0640, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9729, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1981, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7405, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0129, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7952, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1981, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3895, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4631, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8902, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1193, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1707, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7784, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6269, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3652, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4430, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4197, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2590, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3430, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8554, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1673, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0636, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2179, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5896, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3121, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2864, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9067, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2203, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2189, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2521, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2930, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3866, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9245, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2275, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3567, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5181, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8539, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2425, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8690, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2186, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1903, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9398, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4704, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0921, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1794, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3992, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6730, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9108, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3997, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1516, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8581, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2423, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5292, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8260, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2408, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5518, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5883, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3113, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9103, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8735, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2158, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4854, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8498, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2080, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7976, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8278, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2748, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8479, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1814, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3842, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9267, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2944, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4949, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8007, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1347, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1260, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0576, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2946, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9319, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8465, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2420, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3485, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2424, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3091, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8051, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1819, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4192, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6106, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8427, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4122, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8739, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1204, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7608, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8225, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2420, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6326, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2150, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8743, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7960, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8044, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1797, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5193, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1338, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8568, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2502, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2400, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1973, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1004, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2349, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7574, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2986, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2109, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7833, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4028, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8172, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8377, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1532, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4518, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4213, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8999, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5613, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8679, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8899, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9006, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1144, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7961, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6829, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2206, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4629, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8930, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2322, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1635, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1092, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8566, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2347, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1464, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0481, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9417, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2428, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5011, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9917, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2155, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3300, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2204, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6763, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8715, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1278, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8842, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2595, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2702, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8338, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6813, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1638, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8608, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6603, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7050, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2343, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0228, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7593, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8378, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1850, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7096, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2617, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8160, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1295, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3333, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1898, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6511, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4654, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3945, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8777, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1307, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.7914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0038, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.7969, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8749, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2227, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3148, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1697, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9102, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3296, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4191, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2220, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8593, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5742, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3749, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1210, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8458, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8774, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2401, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1581, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9877, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2865, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8902, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2847, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8634, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1741, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3259, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1885, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8416, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3403, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8156, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1895, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8073, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2160, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3166, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6699, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8440, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1280, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9935, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5646, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2739, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4461, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5570, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7555, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1586, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1918, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9565, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1892, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1975, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8138, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2070, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5574, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2582, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5843, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2035, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2474, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7848, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7631, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8841, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2323, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5454, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4394, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8641, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2421, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1981, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9269, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2743, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5669, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2102, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9053, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1217, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1888, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1492, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8037, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2104, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9404, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3360, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4694, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6385, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1649, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2240, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9660, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6043, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2460, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3677, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9980, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2623, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2493, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0568, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2460, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2538, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4723, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9757, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3972, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8941, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2918, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4039, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5055, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8197, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4355, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4964, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7817, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5746, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6063, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8140, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2341, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3445, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1858, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2234, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8860, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8971, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1763, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0617, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9639, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2373, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8712, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4515, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9048, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1669, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1943, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9708, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3028, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3939, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3656, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8105, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2332, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1966, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6191, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2318, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4213, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2271, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1743, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3376, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8637, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3103, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5888, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1320, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(3.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7915, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1228, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1498, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7740, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2652, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8585, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1925, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1427, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8637, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2238, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3896, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9522, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2900, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4786, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2018, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2301, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3061, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1437, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1843, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1132, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9089, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2247, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2252, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0475, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2340, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2741, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9749, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2849, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4309, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1593, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1279, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3005, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8423, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3096, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8090, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1972, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4604, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8950, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7760, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4293, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9151, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5308, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8622, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1225, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3301, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3081, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8910, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0922, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8632, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2828, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8735, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2641, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5953, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2831, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9200, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1702, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7297, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2953, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5799, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8919, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2533, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2966, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4687, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8888, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2303, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7455, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6925, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8348, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1872, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0737, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0498, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9087, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2240, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6059, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3120, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2162, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9456, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9370, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8922, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8327, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2455, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7023, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7381, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8674, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2394, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9080, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0053, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9407, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2202, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2282, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8783, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2573, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8319, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.3173, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9189, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3275, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8641, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2483, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1377, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5165, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2949, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7987, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5997, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3439, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9361, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2214, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4948, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7785, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9318, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2310, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1214, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8861, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2887, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2654, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1555, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2204, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0953, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0371, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8935, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2624, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4669, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2479, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7087, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8463, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2850, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2775, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3186, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8555, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2747, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3452, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6506, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2482, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8945, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3019, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3927, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2378, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1748, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3068, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7935, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1814, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7342, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8481, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2876, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7041, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2737, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9238, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1675, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1541, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0084, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8855, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2279, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3212, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6152, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1734, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6539, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1785, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2558, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5032, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8941, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3478, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2300, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2973, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4177, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1143, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7782, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0367, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2846, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2693, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9109, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1694, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4908, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8744, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0682, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2159, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9197, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8058, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2064, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1301, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8984, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6032, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8539, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1319, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1940, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2321, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8103, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2516, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1954, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4779, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8149, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1898, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1671, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1907, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3083, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1585, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7884, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1243, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5553, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0418, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8966, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3664, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9698, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2441, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9651, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8744, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8857, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2583, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4679, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2669, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8965, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2363, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6456, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8541, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8846, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2797, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4991, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8481, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2340, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4848, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5130, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9020, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2146, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3237, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4596, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9113, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2603, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8044, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2831, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2748, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5522, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9405, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2274, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3292, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9870, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3227, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4301, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9836, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2348, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2191, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9363, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2810, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1354, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4246, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9631, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2993, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9411, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4341, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3131, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5445, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9554, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1893, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9863, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8021, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7969, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8536, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1616, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1653, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1620, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8128, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2639, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4924, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2996, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7843, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3468, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8575, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3425, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4734, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9010, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3962, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4673, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8620, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2262, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7572, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8100, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2027, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0345, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2397, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0947, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8963, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2270, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7259, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5714, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7783, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1701, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1785, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4958, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1973, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7052, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2026, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.4187, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4976, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8679, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2837, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1608, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8363, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4650, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8496, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2097, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2023, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9825, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3051, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1308, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3937, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8343, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2389, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8757, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2967, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4135, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.7171, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9277, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2281, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6650, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9168, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1407, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2380, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9745, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2949, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2273, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2282, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2290, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9560, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2196, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3543, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9654, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4811, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1603, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8793, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2387, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2975, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1910, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9326, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2900, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8960, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2235, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1735, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4755, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8866, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2179, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2254, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0821, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8655, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2281, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3047, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1904, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1410, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0598, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7544, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1198, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9211, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1824, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2282, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3811, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8342, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1948, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9390, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8358, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2650, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4447, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9224, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6788, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9374, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2636, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2469, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0960, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0130, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8702, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2463, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3830, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8926, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9223, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1498, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3647, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2369, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9383, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1541, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5667, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8793, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2564, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3962, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6195, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8649, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1851, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3778, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4521, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8553, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2622, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6994, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4276, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9088, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2888, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4570, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0093, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1342, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0204, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1513, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8809, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1952, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2279, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1504, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8503, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2207, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1753, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2670, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8932, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7162, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0522, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7766, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1827, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3798, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8839, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3057, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4567, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2282, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1076, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9423, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3073, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1065, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9102, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2912, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7404, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2692, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8971, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2080, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5182, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9930, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3194, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3765, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2216, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5509, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2645, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2811, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3041, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8512, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2471, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3697, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1289, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3089, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6801, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2360, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1501, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9892, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8299, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1284, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3080, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3422, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8711, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1254, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1742, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3723, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9197, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1545, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2274, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3354, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3860, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.1603, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9525, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1404, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7411, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0752, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2519, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3849, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9811, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2591, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2894, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9897, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9713, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1969, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7379, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7948, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5143, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2294, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3639, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4601, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8391, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1942, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8906, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8576, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2955, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4221, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9352, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3071, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8854, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2043, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1116, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9078, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2726, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2145, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3967, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8073, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2203, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1939, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0968, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8059, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2660, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1515, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9187, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7017, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2509, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4647, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3563, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8231, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4656, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8368, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2151, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4114, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9647, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8350, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2116, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2756, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9987, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2452, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8001, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0149, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8516, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1849, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4691, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8041, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2297, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3212, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4961, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2798, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4397, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9279, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8744, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1635, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3577, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2350, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8980, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2879, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3123, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0358, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1607, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7430, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1255, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3490, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8529, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1393, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3844, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8975, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3114, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8642, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5517, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2340, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4842, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4546, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2989, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1085, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9130, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1838, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0475, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2017, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8525, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2123, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4057, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6191, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8910, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2561, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4828, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2502, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9315, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2286, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3424, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3074, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9329, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1730, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3569, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1585, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9263, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2713, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9781, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4570, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2143, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8542, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2439, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5017, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8468, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2162, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6377, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8734, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3469, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4853, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9686, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9654, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9548, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(8.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8712, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1261, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3339, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4472, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7758, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1518, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5455, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8442, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8320, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2005, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2474, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2577, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9415, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2813, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3950, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9353, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2641, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9864, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2473, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.5133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0757, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9360, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3527, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8917, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2157, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2408, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9532, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2187, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1331, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8217, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2687, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4166, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9797, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2297, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0611, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4901, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8631, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2367, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3905, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9322, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2532, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3096, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4972, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9084, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2842, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8970, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3786, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8473, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1837, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1193, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8401, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8303, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5198, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8155, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2244, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4570, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2826, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2542, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8781, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2701, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2412, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5502, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8422, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2151, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2234, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0984, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8767, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2034, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8350, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8898, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0156, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8854, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2980, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9136, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8555, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5622, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0507, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2782, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7076, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8231, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4005, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9217, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0496, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8663, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2208, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4117, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1881, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2293, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8886, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2660, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2186, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1516, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1561, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9281, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7656, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(8.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9471, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5096, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2031, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8449, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1543, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3970, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9272, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3889, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0838, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1717, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7852, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1433, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5948, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8999, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3060, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4872, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3021, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9604, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3305, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3031, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9261, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2372, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9038, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2507, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3248, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3932, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1485, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4614, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0862, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1808, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9208, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2523, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6081, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9969, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9323, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2657, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3877, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6064, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7798, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3091, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2567, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2525, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3044, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2231, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5195, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2828, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2630, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3600, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8719, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9948, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2277, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3297, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0904, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8847, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2048, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3823, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2675, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8929, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4641, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1550, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9516, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3797, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2131, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3082, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4692, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2508, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0084, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2679, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3541, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9075, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3360, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8749, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2507, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0537, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9442, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1943, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2276, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1501, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4441, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4767, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1346, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9012, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0471, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5424, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8737, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2788, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1901, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3230, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8987, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2914, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5238, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8961, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2852, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5805, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0900, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9150, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2962, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6002, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1028, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8244, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1836, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9491, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2719, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0173, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7827, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2937, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2527, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1802, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2335, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7228, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4870, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0565, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8673, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2582, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8868, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6102, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8873, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2573, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1301, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9218, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3086, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4992, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9894, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1255, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5363, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2432, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5998, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9556, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9398, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3056, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3439, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1329, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8017, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6355, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8697, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1549, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5528, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8532, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3276, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9276, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1983, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0179, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8065, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2301, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6920, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2226, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9203, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2426, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1670, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9073, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1857, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3345, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0480, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8859, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2978, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0800, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2211, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9219, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3083, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8978, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2531, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2034, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3078, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9324, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2804, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0458, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9116, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1780, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3305, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9012, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8458, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3676, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3461, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9619, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8363, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2569, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4029, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8527, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2839, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3690, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2861, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1903, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4166, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9035, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3209, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9393, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2318, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1794, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0284, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9308, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3702, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1001, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1249, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9481, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2559, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9689, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8595, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1569, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6783, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2278, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2282, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8868, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4888, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8694, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2052, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2579, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8923, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2043, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1955, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9266, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2518, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0316, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9920, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8826, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4480, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2738, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8265, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2740, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0659, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9056, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2492, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5100, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2430, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8841, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7650, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9565, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7927, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2615, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6144, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8321, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8311, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4839, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8809, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1960, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3864, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7970, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2596, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2008, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3272, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1318, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3239, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2289, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8343, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3110, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6738, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9746, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2249, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8544, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9530, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2603, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4569, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8389, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8582, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2390, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2694, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8655, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2570, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5789, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8731, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2671, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8787, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2458, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0917, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1947, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9262, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1761, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0455, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8789, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6232, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4589, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8922, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2020, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4927, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8905, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2340, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2745, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6385, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8772, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2189, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3780, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1871, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1126, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9446, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2686, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9436, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1978, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5162, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0706, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8978, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2065, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7663, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8627, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2570, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3856, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9597, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8767, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2358, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1320, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7361, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9056, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3440, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5802, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5920, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9015, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1692, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1920, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7809, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3117, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3473, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9575, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2084, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9488, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2895, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1131, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0002, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8855, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2942, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1128, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9967, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8619, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2349, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1021, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8996, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2424, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1511, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1477, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7977, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2684, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4436, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3715, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2157, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9734, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1769, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8233, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2932, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5694, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3190, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9148, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6067, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2568, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8440, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1700, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7502, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4523, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8063, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2439, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1978, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8521, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2730, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0722, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5792, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8575, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2389, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3070, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8986, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2987, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6838, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7687, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7656, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9394, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2561, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8769, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0277, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9150, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3479, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2474, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2407, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9161, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8566, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7412, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7894, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1265, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8194, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2394, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6570, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5237, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8554, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1363, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6106, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4128, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8770, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8602, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4219, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1861, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3079, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9882, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8126, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2482, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4009, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2176, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9604, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2655, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3533, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7445, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9270, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2580, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4468, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2131, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8975, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0868, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3421, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8123, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2697, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0929, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1784, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9001, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1259, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0779, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8130, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2205, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2686, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2420, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3236, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5381, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9058, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2442, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1379, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8175, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9472, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0571, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2655, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2001, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8665, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8594, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7776, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2215, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4595, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8790, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3411, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3463, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8047, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1615, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4012, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7747, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1926, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8231, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6201, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8886, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3338, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8120, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6419, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2782, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8906, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2265, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4734, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1156, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8329, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2073, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2909, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2741, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8937, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2556, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2742, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8449, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1939, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4377, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9353, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2713, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0005, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9853, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3100, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4300, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4401, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3416, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3046, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9027, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2340, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0961, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1596, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8443, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2981, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2915, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9091, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2320, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6263, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5382, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9067, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2499, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2429, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6380, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8954, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6715, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1810, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8748, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1469, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6454, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8370, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1355, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4694, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2900, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1259, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2176, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5154, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2556, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3288, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5049, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8573, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1686, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9239, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8668, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3261, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0615, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3399, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8239, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3289, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5858, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8650, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2337, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3406, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8781, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1486, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6361, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2553, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3215, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2351, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8786, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2714, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2850, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2007, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8710, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1444, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9788, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0916, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2431, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8172, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2131, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3027, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1365, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2397, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3339, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4673, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1676, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8454, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.4201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4574, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-14., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9298, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2928, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4622, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5047, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8932, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1265, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1701, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8333, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1891, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8878, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3471, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8909, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2674, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3577, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8223, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8959, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1580, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6103, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3890, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8927, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3063, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1283, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2309, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3468, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4616, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9035, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3285, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7658, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4789, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8746, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1763, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6455, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3449, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9524, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3280, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1660, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8260, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1934, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0619, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8974, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1712, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2677, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1671, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8705, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2571, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2710, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2465, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8824, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2722, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2772, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8838, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3248, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7232, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3148, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8494, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4844, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1431, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2560, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3842, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2627, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8807, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2662, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2665, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8101, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9184, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3177, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3903, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8464, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9522, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3684, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5396, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4195, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9686, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2825, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3473, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2606, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2185, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9895, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2604, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8690, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3185, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3582, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9469, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3210, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7591, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2701, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9209, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3146, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3121, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8596, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4920, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2932, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8121, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1265, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1616, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3445, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8701, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2607, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0581, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3824, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9217, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2583, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5754, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8737, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2630, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1174, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1276, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7744, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2358, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5551, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9111, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1976, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4634, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8899, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2725, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3235, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8237, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4139, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8921, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1758, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3434, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2075, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9197, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2385, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5673, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5611, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9467, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2580, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0726, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9163, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2683, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0022, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1095, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5., device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9816, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2930, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5666, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3727, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8564, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4023, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4273, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9118, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5616, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0778, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2480, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5458, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5980, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1508, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2259, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8168, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2306, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6336, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5162, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9320, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1194, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8970, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2619, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6703, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0693, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8330, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1587, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3624, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4024, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3366, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5669, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8770, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2606, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1620, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8964, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1672, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4319, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0956, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3492, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3017, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4356, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8877, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2350, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3221, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9452, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2521, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2371, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8494, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1667, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4731, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2441, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4009, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3563, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8467, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1261, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9819, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8807, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9031, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3228, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9495, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3211, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9768, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8358, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2608, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3400, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8845, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2381, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1821, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9472, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9204, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2305, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1770, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4697, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8510, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2329, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2252, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6304, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5950, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1391, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8620, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2881, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1721, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0472, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8142, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1280, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2968, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2028, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4016, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4876, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8403, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1260, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4318, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8545, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1039, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8381, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2946, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8472, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2429, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1060, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9220, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3502, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6985, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9117, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1618, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1061, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1262, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2550, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1745, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2658, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1470, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1869, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7175, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2686, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6082, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3587, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9164, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3066, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6059, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8643, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1994, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6912, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3620, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2633, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9066, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-3.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8389, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1827, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9911, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3467, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9024, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2642, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9406, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4952, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8638, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2650, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4690, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0690, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9831, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2149, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8858, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0135, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8574, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4305, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3057, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9569, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1795, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9892, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9149, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2707, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2166, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0707, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8151, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1259, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.2256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0357, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2886, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0130, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8377, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7897, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1728, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8285, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2759, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9683, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3282, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1306, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1518, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.7614, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.7656, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1653, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3036, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4093, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8696, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1132, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9572, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3388, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2298, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1524, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-13.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1252, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7616, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8766, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8592, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1651, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5573, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3124, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8201, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1293, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3872, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4830, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8321, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4075, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8601, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2401, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3725, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8635, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3118, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4050, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2434, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9538, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3127, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1009, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8943, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2453, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4413, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2822, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(5.0025, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2457, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2081, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7049, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9066, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6600, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6052, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8454, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5720, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8928, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2602, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1421, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9844, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2817, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2475, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3127, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8599, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3530, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4705, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9588, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2848, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4397, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2006, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8824, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2893, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3806, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4848, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9349, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2536, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1481, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8395, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2820, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1727, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8170, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3689, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3287, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9426, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4458, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0809, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8912, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2519, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0672, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8887, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2832, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0813, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3573, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8951, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2911, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5236, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8322, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2337, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1237, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2068, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7227, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9211, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8229, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2299, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2823, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1775, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9280, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2420, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0482, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8587, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1954, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2580, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5771, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9092, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2653, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5299, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3955, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8534, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4208, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4194, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8205, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1772, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5300, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2743, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9356, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3222, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6357, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6746, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8689, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7957, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0257, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9115, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2316, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8546, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2745, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5637, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8414, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1923, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3554, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4100, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8592, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1880, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.2125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3695, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8939, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.9540, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8210, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0094, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2482, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8366, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4621, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3440, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2709, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3300, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9074, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2607, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.1367, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2737, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2137, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5181, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0468, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9253, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2001, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4640, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8675, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2152, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2583, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2054, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1788, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2145, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8735, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2553, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3338, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5146, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8818, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3624, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8700, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4296, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4902, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8526, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3008, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7561, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1445, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8693, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2424, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8685, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5016, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5078, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8436, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1417, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6384, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8946, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2333, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2080, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2130, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9523, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2520, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4711, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9961, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9215, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3703, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0996, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9644, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4803, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7506, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9623, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3158, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4633, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9648, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8714, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3365, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8531, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2510, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9992, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3213, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1958, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9645, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2501, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1644, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8556, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2223, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0028, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4787, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9085, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2583, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2605, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1702, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8220, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2060, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1594, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1850, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4402, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1550, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9091, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2315, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3149, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2225, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8947, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2611, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4698, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6277, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8549, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2374, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.8723, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8679, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4810, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9908, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2741, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2483, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4443, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9140, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2329, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8408, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9033, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2696, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6264, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3979, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9815, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2330, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2474, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9408, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3522, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5926, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1627, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8872, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2832, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4304, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3007, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8116, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1273, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4121, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8356, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5158, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9008, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2172, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1661, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9840, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3256, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4757, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4466, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8011, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3950, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4218, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8240, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2326, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1904, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0058, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8466, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2058, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9891, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8235, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2440, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5999, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7072, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8514, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1814, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5404, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0805, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9225, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3885, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9269, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8606, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2325, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5791, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2803, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2898, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4297, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2704, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0326, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3184, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0478, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2386, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8641, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1675, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5167, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6272, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8322, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2661, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5113, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7642, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8372, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2085, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4682, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2355, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0123, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2302, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6162, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1919, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8595, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2436, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5337, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1422, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9076, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4880, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0876, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9026, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2201, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1764, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2449, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8920, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1370, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7901, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1480, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9314, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3338, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4626, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2417, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9242, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2674, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8096, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3229, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2783, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9999, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9451, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2751, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0678, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1115, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5873, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2968, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8965, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1689, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6116, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6437, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9680, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2629, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0951, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3962, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8023, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1718, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6773, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7488, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9012, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3162, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0762, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1803, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9166, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3040, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3245, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5968, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9389, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2083, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2910, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7876, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2354, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3570, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3038, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8284, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1468, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1224, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4714, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9112, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3058, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7766, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1826, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4405, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5031, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8409, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2784, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3810, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0747, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1855, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6526, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6182, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9972, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3013, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2837, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1664, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8662, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3232, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7407, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1489, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4554, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.8875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9163, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0072, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3598, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9033, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1873, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3084, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8107, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2138, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3368, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2432, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7967, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2624, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1043, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9390, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2343, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7225, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2476, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8974, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1268, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3483, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3583, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9132, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1874, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2043, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1599, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1922, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1488, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0534, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8649, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2016, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3264, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2929, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9609, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1706, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4272, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1775, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8881, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1594, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3601, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9990, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8992, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1287, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0866, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0589, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9264, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1777, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1753, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9146, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2446, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7543, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9518, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9531, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8869, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2313, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2428, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1022, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8789, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5362, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9454, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2551, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3928, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1491, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8275, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2146, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4436, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9025, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2740, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3069, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4291, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8064, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2289, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7495, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3969, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9728, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2428, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1752, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1610, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9218, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2317, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4400, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9287, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8362, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2261, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1992, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0634, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7246, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1266, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5359, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2917, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8331, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2623, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4331, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8489, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2573, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.8302, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2648, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8706, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2323, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2344, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4392, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9183, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2115, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9133, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0690, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9167, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2160, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6230, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3670, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7936, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1827, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2202, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8977, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1621, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3636, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4447, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9466, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2912, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6349, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8029, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2323, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7461, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5183, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1562, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9447, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2395, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2760, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0276, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9550, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2367, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1226, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0876, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7987, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3079, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2795, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2908, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1888, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-5.0153, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1698, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8944, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2311, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9982, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8848, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1988, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2222, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3509, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8882, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2212, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7195, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1957, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9340, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2819, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6394, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8390, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2906, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5612, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2198, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9663, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3140, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5622, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2128, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9504, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2980, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2173, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0860, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9373, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2444, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9717, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8387, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2367, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.4207, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4147, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9196, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2275, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6376, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0144, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2575, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7297, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3535, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4688, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7652, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1887, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0893, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3797, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9382, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3037, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6152, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0362, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8183, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2361, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0658, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1889, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8535, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2715, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1619, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3689, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-5.0312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9385, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1868, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7225, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2626, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2500, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8756, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2263, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0025, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1201, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9431, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2402, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2035, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4298, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2635, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2990, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0538, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6562, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9553, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2867, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3991, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9656, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8604, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2814, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0505, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5840, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8175, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2245, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0997, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2723, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9135, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3410, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4378, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2090, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7918, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4670, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8610, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2422, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3697, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4558, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8213, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2815, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2184, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8190, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.9375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8912, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2324, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6337, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5287, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8091, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2373, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3166, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9722, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8965, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3142, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2766, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1209, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8859, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2383, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5208, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0764, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5312, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8224, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2559, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1582, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2859, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9046, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2054, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6206, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0006, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9613, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3274, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3883, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0845, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8125, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(1.0148, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2922, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-3.9664, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4507, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-3.9688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8672, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2009, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2628, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0003, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9406, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2720, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1712, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8994, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2314, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5692, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6430, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9268, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2936, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2125, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6839, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(12.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.7928, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1954, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5405, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1783, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0938, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.8750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9323, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2227, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0501, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.4959, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6875, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9299, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3456, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.2750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.8750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.6250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8316, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1494, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.7725, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3353, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.5000, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8776, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.3014, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2629, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9453, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2663, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5515, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.5271, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.5312, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.1250, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8681, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1267, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2931, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.9254, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.9219, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(5.0625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8914, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2422, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2476, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.9993, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(5., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4., device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8792, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.1435, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3983, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(3.8425, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(3.8438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.8125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2812, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9519, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2328, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1173, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4688, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.9025, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2571, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2116, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.1233, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.9375, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8799, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2925, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.2826, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.6429, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.2812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.6562, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(11.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5625, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3438, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8741, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2625, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.6089, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3178, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.6250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.2500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.4375, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.7188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8351, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2384, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.1978, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.7893, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.7812, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.4062, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8440, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2615, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3352, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0969, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3438, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-9.8750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.6875, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.3750, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8929, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2461, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.5040, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.3055, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.3125, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1875, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.0938, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8829, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2485, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.0563, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0635, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-10.4375, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(10.1250, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "before dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "before dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "after dtype conversion: hidden_states.min() tensor(-4.7500, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after dtype conversion: hidden_states.max() tensor(4.2188, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after variance calculation: variance.min() tensor(0.8581, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after variance calculation: variance.max() tensor(1.2309, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after rsqrt calculation: hidden_states.min() tensor(-4.3684, device='cuda:0', grad_fn=<MinBackward1>)\n",
      "after rsqrt calculation: hidden_states.max() tensor(4.0724, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "after dtype backward conversion: hidden_states.min() tensor(-4.3750, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "after dtype backward conversion: hidden_states.max() tensor(4.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n",
      "self.weight.min() tensor(0.0417, device='cuda:0', dtype=torch.bfloat16)\n",
      "self.weight.max() tensor(2.9219, device='cuda:0', dtype=torch.bfloat16)\n",
      "output: output.min() tensor(-11.0625, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MinBackward1>)\n",
      "output: output.max() tensor(9.5000, device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    max_grad_norm=MAX_GRAD_NORN,\n",
    "    seed=RANDOM_SEED,\n",
    "    bf16=DTYPE == torch.bfloat16,\n",
    "    fp16=DTYPE == torch.float16,\n",
    "    remove_unused_columns=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=1,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds_embeddings_test,\n",
    "    data_collator=collate_fn,\n",
    "    #callbacks=[\n",
    "    #    PrintGradNormCallback()\n",
    "    #],\n",
    ")\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
