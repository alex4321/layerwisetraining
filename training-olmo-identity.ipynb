{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoConfig, OlmoForCausalLM, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from data import dolma\n",
    "from layerwisetrain.identity_model import IdentityWrapper\n",
    "from layerwisetrain.dataset import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "could not get source code",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlayerwisetrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01midentity_model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01minspect\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m src = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetsource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayerwisetrain\u001b[49m\u001b[43m.\u001b[49m\u001b[43midentity_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/inspect.py:1285\u001b[39m, in \u001b[36mgetsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m   1280\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the text of the source code for an object.\u001b[39;00m\n\u001b[32m   1281\u001b[39m \n\u001b[32m   1282\u001b[39m \u001b[33;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[32m   1283\u001b[39m \u001b[33;03m    or code object.  The source code is returned as a single string.  An\u001b[39;00m\n\u001b[32m   1284\u001b[39m \u001b[33;03m    OSError is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     lines, lnum = \u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(lines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/inspect.py:1267\u001b[39m, in \u001b[36mgetsourcelines\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1259\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[32m   1260\u001b[39m \n\u001b[32m   1261\u001b[39m \u001b[33;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1264\u001b[39m \u001b[33;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[32m   1265\u001b[39m \u001b[33;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[32m   1266\u001b[39m \u001b[38;5;28mobject\u001b[39m = unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m lines, lnum = \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1269\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28mobject\u001b[39m = \u001b[38;5;28mobject\u001b[39m.tb_frame\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/inspect.py:1096\u001b[39m, in \u001b[36mfindsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1094\u001b[39m     lines = linecache.getlines(file)\n\u001b[32m   1095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lines:\n\u001b[32m-> \u001b[39m\u001b[32m1096\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mcould not get source code\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   1098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ismodule(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lines, \u001b[32m0\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: could not get source code"
     ]
    }
   ],
   "source": [
    "import layerwisetrain.identity_model\n",
    "import inspect\n",
    "src = inspect.getsource(layerwisetrain.identity_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "BASE_MODEL = \"amd/AMD-OLMo-1B\"\n",
    "BATCH_SIZE = 8\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "TEST_SIZE = 0.01\n",
    "LR = 1e-4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "EPOCHS = 3\n",
    "EVALUATION_FREQUENCY = 0.1\n",
    "WARMUP_RATIO = 0.1\n",
    "EARLY_STOPPING_PATIENCE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, token=HF_TOKEN)\n",
    "config = AutoConfig.from_pretrained(BASE_MODEL, token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OlmoForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "DOLMA_DOWNLOAD_PATH = \"/mnt/ssd/data/dolma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolma_urls = dolma.get_dolma_urls(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolma_fname = dolma.download_dolma_archive(dolma_urls[0], DOLMA_DOWNLOAD_PATH)\n",
    "dolma_dataset = dolma.load_dolma_dataset(dolma_fname)\n",
    "dolma_dataset = dolma.tokenize_dataset(dolma_dataset, tokenizer, config.max_position_embeddings)\n",
    "#dolma_dataset = dolma.shuffle_length_groups(dolma_dataset, minibatch_size=BATCH_SIZE, random_seed=RANDOM_SEED)\n",
    "dolma_dataset = dolma.train_test_split(dolma_dataset, test_size=0.1, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dolma_dataset = dolma_dataset.map(lambda x: dict(**x, labels=x[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_identity = IdentityWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdentityWrapper(\n",
       "  (model): OlmoForCausalLM(\n",
       "    (model): OlmoModel(\n",
       "      (embed_tokens): Embedding(50304, 2048, padding_idx=1)\n",
       "      (layers): ModuleList(\n",
       "        (0-15): 16 x IdentityLayer()\n",
       "      )\n",
       "      (norm): OlmoLayerNorm()\n",
       "      (rotary_emb): OlmoRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=2048, out_features=50304, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_identity.to(\"cuda\")\n",
    "model_identity.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = len(dolma_dataset['train']) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "steps_per_eval = int(steps_per_epoch * EVALUATION_FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"model-identity\",\n",
    "    overwrite_output_dir=False,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=False,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=steps_per_eval,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    eval_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LR,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    lr_scheduler_type='linear',\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_dir='model-identity-logs',\n",
    "    logging_strategy='steps',\n",
    "    logging_steps=1,\n",
    "    save_strategy='steps',\n",
    "    save_steps=steps_per_eval,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    bf16_full_eval=True,\n",
    "    fp16_full_eval=False,\n",
    "    group_by_length=True,\n",
    "    length_column_name='length',\n",
    "    skip_memory_metrics=False,\n",
    "    metric_for_best_model='loss',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16286/1131270099.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_identity,\n",
    "    args=args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=dolma_dataset['train'],\n",
    "    eval_dataset=dolma_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(early_stopping_patience=EARLY_STOPPING_PATIENCE)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1229' max='36852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1229/36852 41:33 < 20:06:41, 0.49 it/s, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1228</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.000016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:2627\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.epoch = epoch + (step + \u001b[32m1\u001b[39m + steps_skipped) / steps_in_epoch\n\u001b[32m   2626\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2627\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2637\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2638\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_substep_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3094\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3096\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3097\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3099\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3044\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3048\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:4147\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4144\u001b[39m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[32m   4145\u001b[39m \u001b[38;5;28mself\u001b[39m._memory_tracker.start()\n\u001b[32m-> \u001b[39m\u001b[32m4147\u001b[39m eval_dataloader = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4148\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_fsdp_xla_v2_enabled:\n\u001b[32m   4149\u001b[39m     eval_dataloader = tpu_spmd_dataloader(eval_dataloader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:1124\u001b[39m, in \u001b[36mTrainer.get_eval_dataloader\u001b[39m\u001b[34m(self, eval_dataset)\u001b[39m\n\u001b[32m   1115\u001b[39m dataloader_params = {\n\u001b[32m   1116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.args.eval_batch_size,\n\u001b[32m   1117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcollate_fn\u001b[39m\u001b[33m\"\u001b[39m: data_collator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpersistent_workers\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.args.dataloader_persistent_workers,\n\u001b[32m   1121\u001b[39m }\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, torch.utils.data.IterableDataset):\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     dataloader_params[\u001b[33m\"\u001b[39m\u001b[33msampler\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_eval_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1125\u001b[39m     dataloader_params[\u001b[33m\"\u001b[39m\u001b[33mdrop_last\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.args.dataloader_drop_last\n\u001b[32m   1126\u001b[39m     dataloader_params[\u001b[33m\"\u001b[39m\u001b[33mprefetch_factor\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.args.dataloader_prefetch_factor\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer.py:1066\u001b[39m, in \u001b[36mTrainer._get_eval_sampler\u001b[39m\u001b[34m(self, eval_dataset)\u001b[39m\n\u001b[32m   1062\u001b[39m         lengths = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1063\u001b[39m     model_input_name = (\n\u001b[32m   1064\u001b[39m         \u001b[38;5;28mself\u001b[39m.processing_class.model_input_names[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1065\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLengthGroupedSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43meval_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.world_size <= \u001b[32m1\u001b[39m:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m SequentialSampler(eval_dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:644\u001b[39m, in \u001b[36mLengthGroupedSampler.__init__\u001b[39m\u001b[34m(self, batch_size, dataset, lengths, model_input_name, generator)\u001b[39m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    637\u001b[39m         \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[32m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset[\u001b[32m0\u001b[39m], BatchEncoding))\n\u001b[32m    638\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m model_input_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[32m0\u001b[39m]\n\u001b[32m    639\u001b[39m     ):\n\u001b[32m    640\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    641\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCan only automatically infer lengths for datasets whose items are dictionaries with an \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    642\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_input_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m key.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    643\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     lengths = \u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_input_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lengths, torch.Tensor):\n\u001b[32m    646\u001b[39m     logger.info(\n\u001b[32m    647\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIf lengths is a torch.Tensor, LengthGroupedSampler will be slow. Converting lengths to List[int]...\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    648\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/datasets/arrow_dataset.py:2387\u001b[39m, in \u001b[36mDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2385\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable.num_rows):\n\u001b[32m   2386\u001b[39m             pa_subtable_ex = pa_subtable.slice(i, \u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2387\u001b[39m             formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2388\u001b[39m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2389\u001b[39m \u001b[43m                \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2390\u001b[39m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2391\u001b[39m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2392\u001b[39m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2393\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2394\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[32m   2395\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/datasets/formatting/formatting.py:658\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    656\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/datasets/formatting/formatting.py:411\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    409\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table, query_type: \u001b[38;5;28mstr\u001b[39m) -> Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    412\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    413\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/datasets/formatting/formatting.py:459\u001b[39m, in \u001b[36mPythonFormatter.format_row\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lazy:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m row = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_row(row)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/layerwisetraining/lib/python3.12/site-packages/datasets/formatting/formatting.py:145\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_row\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "layerwisetraining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
